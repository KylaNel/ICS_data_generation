{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XpnS3F54NaTT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-27 13:14:42.529389: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-02-27 13:14:42.553078: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-27 13:14:42.553102: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-27 13:14:42.553119: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-27 13:14:42.557477: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-02-27 13:14:42.558086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-27 13:14:43.296008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ir3cCqLKNaTT"
      },
      "outputs": [],
      "source": [
        "attacks = pd.read_csv(\"/home/knel/virtual_envs/ankh-morpork/ICS_data_generation/data/swat_processed_sample.csv\", sep=\",\", usecols=range(1,23), skiprows=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PlGx2v1BNaTT"
      },
      "outputs": [],
      "source": [
        "attacks = preprocessing.normalize(attacks.to_numpy(), norm=\"max\", axis=0)\n",
        "\n",
        "# swap sport to front\n",
        "attacks[:, [0, 1]] = attacks[:, [1, 0]]\n",
        "# swap dport to after sport\n",
        "attacks[:, [1, 2]] = attacks[:, [2, 1]]\n",
        "# swap protocols to after dport\n",
        "attacks[:, [2, 14]] = attacks[:, [14, 2]]\n",
        "\n",
        "# column order now -> sport, dport, protocols, continuous (discrete, discrete, discrete, continuous)\n",
        "\n",
        "\n",
        "\n",
        "# should probably add batch and shuffle\n",
        "\n",
        "train_dataset = attacks[:int(np.floor(attacks.shape[0]*3/4))]\n",
        "test_dataset = attacks[int(np.floor(attacks.shape[0]*3/4)):]\n",
        "\n",
        "num_features = attacks[:int(np.floor(attacks.shape[0]*3/4))].shape[1]\n",
        "seq_length = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifqu0pgzNaTT",
        "outputId": "2ec21186-565b-4bc1-e717-e15a2ba5dea6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1500, 500)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRR0cUA17sWo",
        "outputId": "7f31e18a-f1c4-4b54-de4a-1f4173faa25e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([7.27481853e-01, 7.99678830e-01, 2.50000000e-01, 1.33689840e-03,\n",
              "       2.29095074e-03, 1.30282912e-03, 1.94383728e-03, 5.15252969e-04,\n",
              "       1.34779820e-04, 5.58035706e-04, 1.02306549e-03, 1.52555301e-03,\n",
              "       7.65110941e-04, 4.49197871e-01, 2.15040043e-03, 2.15040043e-03,\n",
              "       0.00000000e+00, 2.15040043e-03, 2.15040043e-03, 2.15040043e-03,\n",
              "       0.00000000e+00, 2.15040043e-03])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTZSt9lp71rK",
        "outputId": "ae479dd0-cd5f-4cfd-cc3f-2b345b0c7a70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5.53460684e-01, 6.60183781e-04, 2.50000000e-01, 2.67379679e-03,\n",
              "       1.90912562e-03, 5.24761820e-03, 5.48529191e-03, 5.84350684e-05,\n",
              "       1.01134633e-05, 3.20936160e-05, 2.22186570e-05, 2.28832952e-03,\n",
              "       3.06044376e-03, 7.43833008e-01, 8.10130162e-02, 8.10130162e-02,\n",
              "       0.00000000e+00, 8.10130162e-02, 8.10130162e-02, 8.10130162e-02,\n",
              "       0.00000000e+00, 8.10130162e-02])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FZs594sNaTU"
      },
      "outputs": [],
      "source": [
        "# custom loss\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.keras.backend.mean(y_true * y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPAYKjg2NaTU"
      },
      "outputs": [],
      "source": [
        "# clip model weights to a given hypercube\n",
        "class ClipConstraint(tf.keras.constraints.Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        "\n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn tf.keras.backend.clip(weights, -self.clip_value, self.clip_value)\n",
        "\n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MssgqvzmNaTU"
      },
      "outputs": [],
      "source": [
        "# noise will have same dimension as one flow\n",
        "\n",
        "def make_generator_model(input_dim):\n",
        "    \"\"\"\n",
        "    The generator takes noise as input and goes through a couple\n",
        "    of hidden layers. Then, it splits the output into 3 softmax layers\n",
        "    (one for each categorical variable with the same dimension as the\n",
        "    number of possible values that variable can take) and a dense\n",
        "    layer for all the rest of the continuous variables.\n",
        "\n",
        "    Still experimenting with whether to compile the model here or not.\n",
        "    \"\"\"\n",
        "    input = tf.keras.layers.Input(shape=input_dim, name=\"generator input\")\n",
        "    hidden = tf.keras.layers.Dense(80, activation=\"relu\")(input)\n",
        "    hidden = tf.keras.layers.Dense(80, activation=\"relu\")(hidden)\n",
        "\n",
        "    sport_hidden = tf.keras.layers.Dense(23030, name=\"sport_hidden\")(hidden)\n",
        "    dport_hidden = tf.keras.layers.Dense(14372, name=\"dport_hidden\")(hidden)\n",
        "    proto_hidden = tf.keras.layers.Dense(7, name=\"proto_hidden\")(hidden)\n",
        "\n",
        "    # dense output for continuous, softmax for categorical\n",
        "    sport_output = tf.keras.layers.Softmax(1, name=\"sport_output\")(sport_hidden)\n",
        "    dport_output = tf.keras.layers.Softmax(1, name=\"dport_output\")(dport_hidden)\n",
        "    proto_output = tf.keras.layers.Softmax(1, name=\"proto_output\")(proto_hidden)\n",
        "    cont_output = tf.keras.layers.Dense(num_features-3, name=\"cont_output\")(hidden)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input, outputs=[sport_output, dport_output, proto_output, cont_output],\n",
        "                           name=\"Generator\")\n",
        "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
        "    # model.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDmmHiRtojdW"
      },
      "outputs": [],
      "source": [
        "def make_simple_generator(input_dim):\n",
        "    model = tf.keras.Sequential(name=\"SimpleGenerator\")\n",
        "    model.add(tf.keras.layers.InputLayer(input_shape=(input_dim,)))\n",
        "    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(tf.keras.layers.Dense(num_features, activation=\"linear\"))\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLfrs_rrAOZ8"
      },
      "outputs": [],
      "source": [
        "def convert_generator_output(arr):\n",
        "    \"\"\"\n",
        "    Convert the output of the generator into the format expected for training.\n",
        "    This is so the generated data has the same shape as the real data.\n",
        "    \"\"\"\n",
        "    num_samples = arr.shape[0]\n",
        "    final = np.zeros((num_samples, 22))  # Assuming 22 is the desired output size\n",
        "\n",
        "    # Split the generator output into sport, dport, proto, and cont\n",
        "    sport_dist = arr[:, :23030]\n",
        "    dport_dist = arr[:, 23030:23030 + 14372]\n",
        "    proto_dist = arr[:, 23030 + 14372:23030 + 14372 + 7]\n",
        "    cont = arr[:, 23030 + 14372 + 7:]\n",
        "\n",
        "    # Assign the values to the final array\n",
        "    final[:, :23030] = sport_dist\n",
        "    final[:, 23030:23030 + 14372] = dport_dist\n",
        "    final[:, 23030 + 14372:23030 + 14372 + 7] = proto_dist\n",
        "    final[:, 23030 + 14372 + 7:] = cont\n",
        "\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPIWyZN0NaTU"
      },
      "outputs": [],
      "source": [
        "def convert_generator_output(arr):\n",
        "    \"\"\"\n",
        "    For each categorical feature, takes the most likely probability given\n",
        "    by the Softmax output and concatenates it with the continuous features.\n",
        "    This is so the generated data has the same shape as the real data.\n",
        "    \"\"\"\n",
        "    sport_dist, dport_dist, proto_dist, cont = arr\n",
        "    num_samples = sport_dist.shape[0]\n",
        "    final = np.zeros(22)\n",
        "    for sample_index in range(num_samples):\n",
        "        sport = np.array([np.argmax(sport_dist[sample_index])+1])\n",
        "        dport = np.array([np.argmax(dport_dist[sample_index])+1])\n",
        "        proto = np.array([np.argmax(proto_dist[sample_index])+1])\n",
        "        data_point = np.concatenate((sport, dport, proto, cont[sample_index]))\n",
        "        final = np.vstack((final, data_point))\n",
        "    return tf.convert_to_tensor(final[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Sh5u--NaTU"
      },
      "outputs": [],
      "source": [
        "def make_critic_model(input_shape):\n",
        "    \"\"\"\n",
        "    Critic (discriminator) more or less as described by the WGAN people.\n",
        "    The output layer uses linear activation since this is a critic and not\n",
        "    a standard discriminator.\n",
        "    \"\"\"\n",
        "    const = ClipConstraint(0.01)\n",
        "\n",
        "    input = tf.keras.layers.Input(shape=input_shape, name=\"discriminator input\")\n",
        "    hidden = tf.keras.layers.LSTM(100, recurrent_dropout=0.4, return_sequences=True, kernel_constraint=const)(input)\n",
        "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "    hidden = tf.keras.layers.LSTM(100, recurrent_dropout=0.4, kernel_constraint=const)(hidden)\n",
        "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "    output = tf.keras.layers.Dense(1, activation=\"linear\")(hidden)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input, outputs=output, name=\"Critic\")\n",
        "\n",
        "    # opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
        "    # model.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKdIIrNcNaTU"
      },
      "outputs": [],
      "source": [
        "def make_gan(generator, critic, gen_input_dim):\n",
        "    \"\"\"\n",
        "    Would be a convenient way to train the GAN but this doesn't work\n",
        "    because the generator output has 4 tensors (one for each layer)\n",
        "    and the critic expects an input with the same dimension as the real data.\n",
        "\n",
        "    We can't just change the shape of the real data because I'd have to one-hot encode\n",
        "    the categorical variables - these datasets are huge and it would take FOREVER.\n",
        "    \"\"\"\n",
        "    for layer in critic.layers:\n",
        "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "            layer.trainable = False\n",
        "\n",
        "    input = tf.keras.layers.Input(shape=gen_input_dim, name=\"combined input\")\n",
        "    x = generator(input)\n",
        "    x = critic(x)\n",
        "    model = tf.keras.Model(inputs=input, outputs=x)\n",
        "    opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.00005)\n",
        "    model.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opg2ZbibNaTU"
      },
      "outputs": [],
      "source": [
        "# Some helper functions that may or may not be useful\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t\"\"\"\n",
        "\tGenerate points in latent space as input for the generator.\n",
        "\t\"\"\"\n",
        "\t# generate points in the latent space\n",
        "\tx_input = np.random.randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t\"\"\"\n",
        "\tSelect real samples.\n",
        "\t\"\"\"\n",
        "\t# choose random instances\n",
        "\tix = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "\t# select data\n",
        "\tX = dataset[ix]\n",
        "\t# refactor real data to have same shape as generated data (4 tensors)\n",
        "\n",
        "\t# generate class labels, -1 for 'real'\n",
        "\ty = -np.ones((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\t\"\"\"\n",
        "\tUse the generator to generate n fake examples, with class labels\n",
        "\t\"\"\"\n",
        "\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = generator.predict(x_input)\n",
        "\tX = convert_generator_output(X)\n",
        "\t# create class labels with 1.0 for 'fake'\n",
        "\ty = np.ones((n_samples, 1))\n",
        "\treturn X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRvc7NspNaTV"
      },
      "outputs": [],
      "source": [
        "# Wasserstein loss for critic\n",
        "def critic_loss(pred_real, pred_fake):\n",
        "    return tf.keras.backend.mean(pred_real * pred_fake)\n",
        "\n",
        "# Wasserstein loss for generator\n",
        "def generator_loss(pred_fake):\n",
        "    return -tf.keras.backend.mean(pred_fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgCI_3rzNaTV"
      },
      "outputs": [],
      "source": [
        "generator_optimiser = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.00005)\n",
        "critic_optimiser = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.00005)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p--J_Q9nNaTV"
      },
      "outputs": [],
      "source": [
        "def train_step(generator, critic, dataset, latent_dim, n_batch=64, n_critic=5):\n",
        "\n",
        "    for _ in range(n_critic):\n",
        "\n",
        "        \"\"\"\n",
        "        Train critic more often than the generator for WGAN\n",
        "        \"\"\"\n",
        "\n",
        "        with tf.GradientTape() as critic_tape:\n",
        "            # Critic training works fine\n",
        "            X_real, y_real = generate_real_samples(dataset, n_batch)\n",
        "            noise = generate_latent_points(latent_dim, n_batch)\n",
        "            X_fake = generator(noise, training=False)\n",
        "            X_fake = convert_generator_output(X_fake)\n",
        "\n",
        "            pred_real = critic(X_real, training=True)\n",
        "            pred_fake = critic(X_fake, training=True)\n",
        "\n",
        "            c_loss = critic_loss(pred_real, pred_fake)\n",
        "\n",
        "        critic_gradients = critic_tape.gradient(c_loss, critic.trainable_variables)\n",
        "        critic_optimiser.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
        "    print(\"In here after discriminator\")\n",
        "    \"\"\"\n",
        "    Train generator\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        X_fake = generator(noise, training=True)\n",
        "        X_fake = convert_generator_output(X_fake)\n",
        "        pred_fake = critic(X_fake, training=False)\n",
        "        g_loss = generator_loss(pred_fake)\n",
        "        print(gen_tape.gradient)\n",
        "    tf.print(\"Generator Loss:\", g_loss)\n",
        "    print(\"Generator Trainable Variables:\", generator.trainable_variables)\n",
        "    asdasd\n",
        "    # These gradients are all None and I can't figure out why\n",
        "    # Are the multiple output layers of the generator a problem here?\n",
        "    # Maybe the definition of the loss function?\n",
        "    generator_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
        "    print(\"Critic Gradients:\", critic_gradients)\n",
        "    print(\"Generator Gradients:\", generator_gradients)\n",
        "    easdasd\n",
        "\n",
        "    generator_optimiser.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "\n",
        "\n",
        "\n",
        "    return c_loss, g_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmOiiNQUIu-s"
      },
      "outputs": [],
      "source": [
        "def train_step(generator, critic, dataset, latent_dim, n_batch=64, n_critic=5):\n",
        "    # 1. Train critic\n",
        "    for _ in range(n_critic):\n",
        "        with tf.GradientTape() as critic_tape:\n",
        "            # Generate real and fake samples\n",
        "            X_real, y_real = generate_real_samples(dataset, n_batch)\n",
        "            noise = generate_latent_points(latent_dim, n_batch)\n",
        "            X_fake = generator(noise, training=False)\n",
        "            X_fake = convert_generator_output(X_fake)\n",
        "\n",
        "            # Compute critic predictions for real and fake samples\n",
        "            pred_real = critic(X_real, training=True)\n",
        "            pred_fake = critic(X_fake, training=True)\n",
        "\n",
        "            # Compute critic loss\n",
        "            c_loss = critic_loss(pred_real, pred_fake)\n",
        "\n",
        "        # Update critic weights\n",
        "        critic_gradients = critic_tape.gradient(c_loss, critic.trainable_variables)\n",
        "        critic_optimiser.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
        "\n",
        "    # Generate noise for the generator training\n",
        "    noise = generate_latent_points(latent_dim, n_batch)\n",
        "\n",
        "    # 2. Train generator\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        # Generate fake samples for the current batch\n",
        "        noise = generate_latent_points(latent_dim, n_batch)\n",
        "        X_fake = generator(noise, training=True)\n",
        "        X_fake = convert_generator_output(X_fake)\n",
        "\n",
        "        # Compute critic predictions for the generated samples\n",
        "        pred_fake = critic(X_fake, training=False)\n",
        "\n",
        "        # Compute generator loss\n",
        "        g_loss = generator_loss(pred_fake)\n",
        "\n",
        "\n",
        "    print(\"Generated Output:\", X_fake)\n",
        "    tf.print(\"Generator Loss:\", g_loss)\n",
        "    print(\"Generator summary \", generator.summary())\n",
        "    print(\"Trainable variables: \", generator.trainable_variables)\n",
        "    # Compute generator gradients\n",
        "    generator_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
        "    print(\"NaN in Generator Output:\", tf.reduce_any(tf.math.is_nan(X_fake)))\n",
        "    print(\"NaN in Generator Loss:\", tf.reduce_any(tf.math.is_nan(g_loss)))\n",
        "    print(\"Min Generator Output:\", tf.reduce_min(X_fake))\n",
        "    print(\"Max Generator Output:\", tf.reduce_max(X_fake))\n",
        "    print(\"Inf in Generator Output:\", tf.reduce_any(tf.math.is_inf(X_fake)))\n",
        "    print(\"Inf in Generator Loss:\", tf.reduce_any(tf.math.is_inf(g_loss)))\n",
        "    for grad, var in zip(generator_gradients, generator.trainable_variables):\n",
        "      print(f\"Gradient for {var.name}:\", grad)\n",
        "    print(\"Gen gradients : \", generator_gradients)\n",
        "    # Update generator weights\n",
        "    generator_optimiser.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "\n",
        "    # Print and return losses\n",
        "    tf.print(\"Critic Loss:\", c_loss)\n",
        "    tf.print(\"Generator Loss:\", g_loss)\n",
        "\n",
        "    return c_loss, g_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0j20okfNaTV"
      },
      "outputs": [],
      "source": [
        "def train(dataset, n_epochs=2, latent_dim=50):\n",
        "\n",
        "    generator = make_generator_model(latent_dim)\n",
        "    generator = make_simple_generator(latent_dim)\n",
        "    critic = make_critic_model((num_features, 1))\n",
        "    generator.compile(optimizer=generator_optimiser, loss=generator_loss)\n",
        "    c_losses, g_losses = [], []\n",
        "\n",
        "    for _ in range(n_epochs):\n",
        "        for batch in dataset:\n",
        "            c_loss, g_loss = train_step(generator, critic, dataset, latent_dim)\n",
        "            c_losses.append(c_loss)\n",
        "            g_losses.append(g_loss)\n",
        "\n",
        "    return c_losses, g_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "earlXKh-NaTV",
        "outputId": "9950d7c2-0593-4c6c-8f1f-8d2bebe36891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Output: [[ 0.03682016 -0.13725437  0.01530193 ...  0.00323969  0.00109265\n",
            "   0.02656257]\n",
            " [ 0.12080162 -0.12529905  0.07918794 ...  0.02806921  0.03629631\n",
            "   0.00331291]\n",
            " [ 0.13487729 -0.18661509  0.02867277 ... -0.03999968 -0.11089537\n",
            "  -0.00321516]\n",
            " ...\n",
            " [ 0.01401116 -0.07144432  0.03908198 ...  0.0269659   0.00047158\n",
            "  -0.00950964]\n",
            " [ 0.06002793 -0.13166794  0.06363656 ...  0.02195042  0.02467676\n",
            "  -0.05509738]\n",
            " [ 0.14546759 -0.14995448  0.0473679  ...  0.04530958  0.0933165\n",
            "  -0.03924917]]\n",
            "Generator Loss: -0.000300596264\n",
            "Model: \"SimpleGenerator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_50 (Dense)            (None, 128)               1408      \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 22)                2838      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 333078 (1.27 MB)\n",
            "Trainable params: 333078 (1.27 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Generator summary  None\n",
            "Trainable variables:  [<tf.Variable 'dense_50/kernel:0' shape=(10, 128) dtype=float32, numpy=\n",
            "array([[-0.16613153, -0.04200545,  0.15716307, ...,  0.15045272,\n",
            "         0.14351399,  0.11724181],\n",
            "       [-0.03295447,  0.2004156 , -0.00451599, ..., -0.1395693 ,\n",
            "        -0.1571418 , -0.04516445],\n",
            "       [ 0.1617022 , -0.06330299, -0.10965457, ..., -0.01647766,\n",
            "         0.04076628, -0.14750217],\n",
            "       ...,\n",
            "       [ 0.10820521,  0.03061028, -0.01889439, ...,  0.14297156,\n",
            "        -0.1828673 , -0.19050944],\n",
            "       [-0.149292  ,  0.05717115, -0.09174094, ..., -0.04277968,\n",
            "        -0.10745165,  0.11147936],\n",
            "       [ 0.0389486 ,  0.16030927, -0.13768798, ...,  0.15474252,\n",
            "         0.17467196, -0.20429356]], dtype=float32)>, <tf.Variable 'dense_50/bias:0' shape=(128,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_51/kernel:0' shape=(128, 256) dtype=float32, numpy=\n",
            "array([[ 0.05907857, -0.08867118, -0.09270173, ..., -0.04897019,\n",
            "         0.09602007,  0.07590324],\n",
            "       [-0.08788669, -0.02938849,  0.0072664 , ..., -0.02927664,\n",
            "         0.11194763,  0.03841326],\n",
            "       [ 0.12317276,  0.09988868, -0.10995719, ..., -0.0938513 ,\n",
            "         0.06134117, -0.0803299 ],\n",
            "       ...,\n",
            "       [ 0.11489233, -0.05070201,  0.10697418, ..., -0.04514003,\n",
            "        -0.07987517,  0.01723203],\n",
            "       [ 0.09778115, -0.01049873, -0.04768631, ..., -0.02634436,\n",
            "        -0.02870098,  0.03121206],\n",
            "       [ 0.07608059, -0.07744327,  0.04006693, ...,  0.07991827,\n",
            "         0.10222244, -0.05806687]], dtype=float32)>, <tf.Variable 'dense_51/bias:0' shape=(256,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0.], dtype=float32)>, <tf.Variable 'dense_52/kernel:0' shape=(256, 512) dtype=float32, numpy=\n",
            "array([[-0.08007612,  0.07435619,  0.04704408, ..., -0.01697363,\n",
            "         0.06055353, -0.05469656],\n",
            "       [ 0.06235277,  0.03415775,  0.04393979, ...,  0.03754006,\n",
            "         0.02874432, -0.01186506],\n",
            "       [ 0.07241366,  0.05037149,  0.01523107, ..., -0.07490833,\n",
            "         0.07001252,  0.02619928],\n",
            "       ...,\n",
            "       [ 0.02668675,  0.00694085,  0.06101725, ..., -0.06715639,\n",
            "         0.00262295,  0.06418762],\n",
            "       [ 0.02039575, -0.03339023, -0.04166307, ..., -0.0655559 ,\n",
            "        -0.02263165,  0.08518969],\n",
            "       [-0.01182782, -0.03369876, -0.06670162, ..., -0.03868958,\n",
            "         0.04278543,  0.00335474]], dtype=float32)>, <tf.Variable 'dense_52/bias:0' shape=(512,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0.], dtype=float32)>, <tf.Variable 'dense_53/kernel:0' shape=(512, 256) dtype=float32, numpy=\n",
            "array([[ 0.03126363, -0.0605646 ,  0.0239329 , ...,  0.05611721,\n",
            "        -0.07696878, -0.06268764],\n",
            "       [ 0.0848917 , -0.01451371,  0.03192754, ...,  0.02789828,\n",
            "        -0.03776039, -0.00802737],\n",
            "       [ 0.01754902, -0.07039896,  0.02102595, ..., -0.06046509,\n",
            "         0.08007676,  0.04878401],\n",
            "       ...,\n",
            "       [-0.08593652, -0.01971748,  0.0352429 , ..., -0.03147537,\n",
            "        -0.0620283 , -0.08414846],\n",
            "       [-0.0691395 ,  0.03711943, -0.05132607, ...,  0.07700493,\n",
            "        -0.06304856, -0.03290963],\n",
            "       [-0.05466522, -0.0713885 , -0.05899859, ...,  0.03182466,\n",
            "        -0.04478665, -0.00723375]], dtype=float32)>, <tf.Variable 'dense_53/bias:0' shape=(256,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0.], dtype=float32)>, <tf.Variable 'dense_54/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
            "array([[-0.01275873,  0.10888672, -0.02582687, ...,  0.09105206,\n",
            "         0.00582442, -0.0641275 ],\n",
            "       [-0.04696748,  0.0438143 ,  0.02088565, ..., -0.0917778 ,\n",
            "        -0.08759394, -0.08184162],\n",
            "       [ 0.10691008,  0.01167148,  0.03383526, ...,  0.06596372,\n",
            "         0.08098307, -0.02645612],\n",
            "       ...,\n",
            "       [-0.06359607, -0.01172379,  0.06863052, ...,  0.05156314,\n",
            "        -0.11191618, -0.00972494],\n",
            "       [-0.07032755, -0.03501824, -0.07486996, ...,  0.06008342,\n",
            "        -0.06097937, -0.11826178],\n",
            "       [ 0.06300762,  0.02919659,  0.04819891, ..., -0.01392093,\n",
            "         0.1155107 ,  0.06852564]], dtype=float32)>, <tf.Variable 'dense_54/bias:0' shape=(128,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_55/kernel:0' shape=(128, 22) dtype=float32, numpy=\n",
            "array([[-0.10347805, -0.0744423 ,  0.13659926, ..., -0.1757793 ,\n",
            "         0.09728222,  0.02666497],\n",
            "       [-0.12029705, -0.13946342,  0.10678367, ..., -0.11870956,\n",
            "        -0.11195078, -0.02473931],\n",
            "       [ 0.07392998, -0.06249733, -0.06618948, ...,  0.16374011,\n",
            "        -0.06404296,  0.12305845],\n",
            "       ...,\n",
            "       [-0.05200544,  0.05174474,  0.17571874, ...,  0.11355333,\n",
            "         0.04667386,  0.08487166],\n",
            "       [-0.0167574 ,  0.06254448,  0.01580659, ..., -0.14545926,\n",
            "        -0.08830037,  0.05419435],\n",
            "       [-0.17059623,  0.18053336, -0.16017528, ..., -0.07192425,\n",
            "         0.08475967,  0.14339496]], dtype=float32)>, <tf.Variable 'dense_55/bias:0' shape=(22,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0.], dtype=float32)>]\n",
            "NaN in Generator Output: tf.Tensor(False, shape=(), dtype=bool)\n",
            "NaN in Generator Loss: tf.Tensor(False, shape=(), dtype=bool)\n",
            "Min Generator Output: tf.Tensor(-0.313684344291687, shape=(), dtype=float64)\n",
            "Max Generator Output: tf.Tensor(0.27994444966316223, shape=(), dtype=float64)\n",
            "Inf in Generator Output: tf.Tensor(False, shape=(), dtype=bool)\n",
            "Inf in Generator Loss: tf.Tensor(False, shape=(), dtype=bool)\n",
            "Gradient for dense_50/kernel:0: None\n",
            "Gradient for dense_50/bias:0: None\n",
            "Gradient for dense_51/kernel:0: None\n",
            "Gradient for dense_51/bias:0: None\n",
            "Gradient for dense_52/kernel:0: None\n",
            "Gradient for dense_52/bias:0: None\n",
            "Gradient for dense_53/kernel:0: None\n",
            "Gradient for dense_53/bias:0: None\n",
            "Gradient for dense_54/kernel:0: None\n",
            "Gradient for dense_54/bias:0: None\n",
            "Gradient for dense_55/kernel:0: None\n",
            "Gradient for dense_55/bias:0: None\n",
            "Gen gradients :  [None, None, None, None, None, None, None, None, None, None, None, None]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "No gradients provided for any variable: (['dense_50/kernel:0', 'dense_50/bias:0', 'dense_51/kernel:0', 'dense_51/bias:0', 'dense_52/kernel:0', 'dense_52/bias:0', 'dense_53/kernel:0', 'dense_53/bias:0', 'dense_54/kernel:0', 'dense_54/bias:0', 'dense_55/kernel:0', 'dense_55/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_50/kernel:0' shape=(10, 128) dtype=float32, numpy=\narray([[-0.16613153, -0.04200545,  0.15716307, ...,  0.15045272,\n         0.14351399,  0.11724181],\n       [-0.03295447,  0.2004156 , -0.00451599, ..., -0.1395693 ,\n        -0.1571418 , -0.04516445],\n       [ 0.1617022 , -0.06330299, -0.10965457, ..., -0.01647766,\n         0.04076628, -0.14750217],\n       ...,\n       [ 0.10820521,  0.03061028, -0.01889439, ...,  0.14297156,\n        -0.1828673 , -0.19050944],\n       [-0.149292  ,  0.05717115, -0.09174094, ..., -0.04277968,\n        -0.10745165,  0.11147936],\n       [ 0.0389486 ,  0.16030927, -0.13768798, ...,  0.15474252,\n         0.17467196, -0.20429356]], dtype=float32)>), (None, <tf.Variable 'dense_50/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_51/kernel:0' shape=(128, 256) dtype=float32, numpy=\narray([[ 0.05907857, -0.08867118, -0.09270173, ..., -0.04897019,\n         0.09602007,  0.07590324],\n       [-0.08788669, -0.02938849,  0.0072664 , ..., -0.02927664,\n         0.11194763,  0.03841326],\n       [ 0.12317276,  0.09988868, -0.10995719, ..., -0.0938513 ,\n         0.06134117, -0.0803299 ],\n       ...,\n       [ 0.11489233, -0.05070201,  0.10697418, ..., -0.04514003,\n        -0.07987517,  0.01723203],\n       [ 0.09778115, -0.01049873, -0.04768631, ..., -0.02634436,\n        -0.02870098,  0.03121206],\n       [ 0.07608059, -0.07744327,  0.04006693, ...,  0.07991827,\n         0.10222244, -0.05806687]], dtype=float32)>), (None, <tf.Variable 'dense_51/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_52/kernel:0' shape=(256, 512) dtype=float32, numpy=\narray([[-0.08007612,  0.07435619,  0.04704408, ..., -0.01697363,\n         0.06055353, -0.05469656],\n       [ 0.06235277,  0.03415775,  0.04393979, ...,  0.03754006,\n         0.02874432, -0.01186506],\n       [ 0.07241366,  0.05037149,  0.01523107, ..., -0.07490833,\n         0.07001252,  0.02619928],\n       ...,\n       [ 0.02668675,  0.00694085,  0.06101725, ..., -0.06715639,\n         0.00262295,  0.06418762],\n       [ 0.02039575, -0.03339023, -0.04166307, ..., -0.0655559 ,\n        -0.02263165,  0.08518969],\n       [-0.01182782, -0.03369876, -0.06670162, ..., -0.03868958,\n         0.04278543,  0.00335474]], dtype=float32)>), (None, <tf.Variable 'dense_52/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_53/kernel:0' shape=(512, 256) dtype=float32, numpy=\narray([[ 0.03126363, -0.0605646 ,  0.0239329 , ...,  0.05611721,\n        -0.07696878, -0.06268764],\n       [ 0.0848917 , -0.01451371,  0.03192754, ...,  0.02789828,\n        -0.03776039, -0.00802737],\n       [ 0.01754902, -0.07039896,  0.02102595, ..., -0.06046509,\n         0.08007676,  0.04878401],\n       ...,\n       [-0.08593652, -0.01971748,  0.0352429 , ..., -0.03147537,\n        -0.0620283 , -0.08414846],\n       [-0.0691395 ,  0.03711943, -0.05132607, ...,  0.07700493,\n        -0.06304856, -0.03290963],\n       [-0.05466522, -0.0713885 , -0.05899859, ...,  0.03182466,\n        -0.04478665, -0.00723375]], dtype=float32)>), (None, <tf.Variable 'dense_53/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_54/kernel:0' shape=(256, 128) dtype=float32, numpy=\narray([[-0.01275873,  0.10888672, -0.02582687, ...,  0.09105206,\n         0.00582442, -0.0641275 ],\n       [-0.04696748,  0.0438143 ,  0.02088565, ..., -0.0917778 ,\n        -0.08759394, -0.08184162],\n       [ 0.10691008,  0.01167148,  0.03383526, ...,  0.06596372,\n         0.08098307, -0.02645612],\n       ...,\n       [-0.06359607, -0.01172379,  0.06863052, ...,  0.05156314,\n        -0.11191618, -0.00972494],\n       [-0.07032755, -0.03501824, -0.07486996, ...,  0.06008342,\n        -0.06097937, -0.11826178],\n       [ 0.06300762,  0.02919659,  0.04819891, ..., -0.01392093,\n         0.1155107 ,  0.06852564]], dtype=float32)>), (None, <tf.Variable 'dense_54/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_55/kernel:0' shape=(128, 22) dtype=float32, numpy=\narray([[-0.10347805, -0.0744423 ,  0.13659926, ..., -0.1757793 ,\n         0.09728222,  0.02666497],\n       [-0.12029705, -0.13946342,  0.10678367, ..., -0.11870956,\n        -0.11195078, -0.02473931],\n       [ 0.07392998, -0.06249733, -0.06618948, ...,  0.16374011,\n        -0.06404296,  0.12305845],\n       ...,\n       [-0.05200544,  0.05174474,  0.17571874, ...,  0.11355333,\n         0.04667386,  0.08487166],\n       [-0.0167574 ,  0.06254448,  0.01580659, ..., -0.14545926,\n        -0.08830037,  0.05419435],\n       [-0.17059623,  0.18053336, -0.16017528, ..., -0.07192425,\n         0.08475967,  0.14339496]], dtype=float32)>), (None, <tf.Variable 'dense_55/bias:0' shape=(22,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0.], dtype=float32)>)).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a8127c9dcb1d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test training on a small sample of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-c49287d35db0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, n_epochs, latent_dim)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mc_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mg_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-32c7cd50d7b2>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(generator, critic, dataset, latent_dim, n_batch, n_critic)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gen gradients : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Update generator weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mgenerator_optimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Print and return losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    709\u001b[0m           \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \"\"\"\n\u001b[0;32m--> 711\u001b[0;31m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;34mf\"No gradients provided for any variable: {variable}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;34mf\"Provided `grads_and_vars` is {grads_and_vars}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['dense_50/kernel:0', 'dense_50/bias:0', 'dense_51/kernel:0', 'dense_51/bias:0', 'dense_52/kernel:0', 'dense_52/bias:0', 'dense_53/kernel:0', 'dense_53/bias:0', 'dense_54/kernel:0', 'dense_54/bias:0', 'dense_55/kernel:0', 'dense_55/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_50/kernel:0' shape=(10, 128) dtype=float32, numpy=\narray([[-0.16613153, -0.04200545,  0.15716307, ...,  0.15045272,\n         0.14351399,  0.11724181],\n       [-0.03295447,  0.2004156 , -0.00451599, ..., -0.1395693 ,\n        -0.1571418 , -0.04516445],\n       [ 0.1617022 , -0.06330299, -0.10965457, ..., -0.01647766,\n         0.04076628, -0.14750217],\n       ...,\n       [ 0.10820521,  0.03061028, -0.01889439, ...,  0.14297156,\n        -0.1828673 , -0.19050944],\n       [-0.149292  ,  0.05717115, -0.09174094, ..., -0.04277968,\n        -0.10745165,  0.11147936],\n       [ 0.0389486 ,  0.16030927, -0.13768798, ...,  0.15474252,\n         0.17467196, -0.20429356]], dtype=float32)>), (None, <tf.Variable 'dense_50/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_51/kernel:0' shape=(128, 256) dtype=float32, numpy=\narray([[ 0.05907857, -0.08867118, -0.09270173, ..., -0.04897019,\n         0.09602007,  0.07590324],\n       [-0.08788669, -0.02938849,  0.0072664 , ..., -0.02927664,\n         0.11194763,  0.03841326],\n       [ 0.12317276,  0.09988868, -0.10995719, ..., -0.0938513 ,\n         0.06134117, -0.0803299 ],\n       ...,\n       [ 0.11489233, -0.05070201,  0.10697418, ..., -0.04514003,\n        -0.07987517,  0.01723203],\n       [ 0.09778115, -0.01049873, -0.04768631, ..., -0.02634436,\n        -0.02870098,  0.03121206],\n       [ 0.07608059, -0.07744327,  0.04006693, ...,  0.07991827,\n         0.10222244, -0.05806687]], dtype=float32)>), (None, <tf.Variable 'dense_51/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_52/kernel:0' shape=(256, 512) dtype=float32, numpy=\narray([[-0.08007612,  0.07435619,  0.04704408, ..., -0.01697363,\n         0.06055353, -0.05469656],\n       [ 0.06235277,  0.03415775,  0.04393979, ...,  0.03754006,\n         0.02874432, -0.01186506],\n       [ 0.07241366,  0.05037149,  0.01523107, ..., -0.07490833,\n         0.07001252,  0.02619928],\n       ...,\n       [ 0.02668675,  0.00694085,  0.06101725, ..., -0.06715639,\n         0.00262295,  0.06418762],\n       [ 0.02039575, -0.03339023, -0.04166307, ..., -0.0655559 ,\n        -0.02263165,  0.08518969],\n       [-0.01182782, -0.03369876, -0.06670162, ..., -0.03868958,\n         0.04278543,  0.00335474]], dtype=float32)>), (None, <tf.Variable 'dense_52/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_53/kernel:0' shape=(512, 256) dtype=float32, numpy=\narray([[ 0.03126363, -0.0605646 ,  0.0239329 , ...,  0.05611721,\n        -0.07696878, -0.06268764],\n       [ 0.0848917 , -0.01451371,  0.03192754, ...,  0.02789828,\n        -0.03776039, -0.00802737],\n       [ 0.01754902, -0.07039896,  0.02102595, ..., -0.06046509,\n         0.08007676,  0.04878401],\n       ...,\n       [-0.08593652, -0.01971748,  0.0352429 , ..., -0.03147537,\n        -0.0620283 , -0.08414846],\n       [-0.0691395 ,  0.03711943, -0.05132607, ...,  0.07700493,\n        -0.06304856, -0.03290963],\n       [-0.05466522, -0.0713885 , -0.05899859, ...,  0.03182466,\n        -0.04478665, -0.00723375]], dtype=float32)>), (None, <tf.Variable 'dense_53/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_54/kernel:0' shape=(256, 128) dtype=float32, numpy=\narray([[-0.01275873,  0.10888672, -0.02582687, ...,  0.09105206,\n         0.00582442, -0.0641275 ],\n       [-0.04696748,  0.0438143 ,  0.02088565, ..., -0.0917778 ,\n        -0.08759394, -0.08184162],\n       [ 0.10691008,  0.01167148,  0.03383526, ...,  0.06596372,\n         0.08098307, -0.02645612],\n       ...,\n       [-0.06359607, -0.01172379,  0.06863052, ...,  0.05156314,\n        -0.11191618, -0.00972494],\n       [-0.07032755, -0.03501824, -0.07486996, ...,  0.06008342,\n        -0.06097937, -0.11826178],\n       [ 0.06300762,  0.02919659,  0.04819891, ..., -0.01392093,\n         0.1155107 ,  0.06852564]], dtype=float32)>), (None, <tf.Variable 'dense_54/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_55/kernel:0' shape=(128, 22) dtype=float32, numpy=\narray([[-0.10347805, -0.0744423 ,  0.13659926, ..., -0.1757793 ,\n         0.09728222,  0.02666497],\n       [-0.12029705, -0.13946342,  0.10678367, ..., -0.11870956,\n        -0.11195078, -0.02473931],\n       [ 0.07392998, -0.06249733, -0.06618948, ...,  0.16374011,\n        -0.06404296,  0.12305845],\n       ...,\n       [-0.05200544,  0.05174474,  0.17571874, ...,  0.11355333,\n         0.04667386,  0.08487166],\n       [-0.0167574 ,  0.06254448,  0.01580659, ..., -0.14545926,\n        -0.08830037,  0.05419435],\n       [-0.17059623,  0.18053336, -0.16017528, ..., -0.07192425,\n         0.08475967,  0.14339496]], dtype=float32)>), (None, <tf.Variable 'dense_55/bias:0' shape=(22,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0.], dtype=float32)>))."
          ]
        }
      ],
      "source": [
        "# Test training on a small sample of the dataset\n",
        "dataset = attacks[:10]\n",
        "c_losses, g_losses = train(dataset, 2, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0GiE8Y2NaTV"
      },
      "outputs": [],
      "source": [
        "# generator = make_generator_model(50)\n",
        "# critic = make_critic_model((num_features, 1))\n",
        "# noise = generate_latent_points(50, 3)\n",
        "# X_fake = generator(noise, training=False)\n",
        "# X_fake = convert_generator_output(X_fake)\n",
        "# y_fake = critic(X_fake)\n",
        "# g_loss = generator_loss(y_fake)\n",
        "\n",
        "# generator.compile(loss=generator_loss, optimizer=generator_optimiser)\n",
        "# gl = generator(noise)\n",
        "# gl = convert_generator_output(gl)\n",
        "# yg = critic(gl)\n",
        "# generator.train_on_batch(noise, yg)\n",
        "\n",
        "# g_loss, yg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8kxAbgBNaTV"
      },
      "outputs": [],
      "source": [
        "# # size of the latent space\n",
        "# latent_dim = 50\n",
        "# # create the critic\n",
        "# critic = make_critic_model((num_features, 1))\n",
        "# # create the generator\n",
        "# generator = make_generator_model(latent_dim)\n",
        "# # create the gan\n",
        "# gan_model = make_gan(generator, critic, latent_dim)\n",
        "# # load image data\n",
        "\n",
        "# epochs = 10\n",
        "# # train model\n",
        "# critic_loss_real, critic_loss_fake, generator_loss = train(generator, critic, dataset, latent_dim, n_epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-_FmJG-NaTV"
      },
      "outputs": [],
      "source": [
        "# plt.plot(critic_loss_fake, label=\"crit\")\n",
        "# plt.plot(generator_loss, label=\"gen\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUP6qnDrNaTV"
      },
      "outputs": [],
      "source": [
        "# # train the generator and critic\n",
        "# def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=2, n_batch=64, n_critic=5):\n",
        "# \t# calculate the number of batches per training epoch\n",
        "# \tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "# \t# calculate the number of training iterations\n",
        "# \tn_steps = bat_per_epo * n_epochs\n",
        "# \t# calculate the size of half a batch of samples\n",
        "# \thalf_batch = int(n_batch / 2)\n",
        "# \t# lists for keeping track of loss\n",
        "# \tc1_hist, c2_hist, g_hist = list(), list(), list()\n",
        "# \t# manually enumerate epochs\n",
        "# \tfor i in range(n_steps):\n",
        "# \t\t# update the critic more than the generator\n",
        "# \t\tc1_tmp, c2_tmp = list(), list()\n",
        "# \t\tfor _ in range(n_critic):\n",
        "# \t\t\t# get randomly selected 'real' samples\n",
        "# \t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "# \t\t\t# update critic model weights\n",
        "# \t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n",
        "# \t\t\tc1_tmp.append(c_loss1)\n",
        "# \t\t\t# generate 'fake' examples\n",
        "# \t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "# \t\t\t# update critic model weights\n",
        "# \t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
        "# \t\t\tc2_tmp.append(c_loss2)\n",
        "# \t\t# store critic loss\n",
        "# \t\tc1_hist.append(np.mean(c1_tmp))\n",
        "# \t\tc2_hist.append(np.mean(c2_tmp))\n",
        "# \t\t# prepare points in latent space as input for the generator\n",
        "# \t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "# \t\tprint(X_gan.shape)\n",
        "# \t\t# create inverted labels for the fake samples\n",
        "# \t\ty_gan = -np.ones((n_batch, 1))\n",
        "# \t\t# update the generator via the critic's error\n",
        "# \t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "# \t\tg_hist.append(g_loss)\n",
        "# \t\t# summarize loss on this batch\n",
        "# \t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n",
        "# \t\tplot_history(c1_hist, c2_hist, g_hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xlMvaNXAMG9F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, _data, num_classes_list):\n",
        "        self.data = torch.tensor(_data, dtype=torch.float32)\n",
        "        self.num_classes_list = num_classes_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # One-hot encode each of the first three values with different num_classes\n",
        "        one_hot_encoded_1 = torch.nn.functional.one_hot(sample[0].long(), self.num_classes_list[0])\n",
        "        one_hot_encoded_2 = torch.nn.functional.one_hot(sample[1].long(), self.num_classes_list[1])\n",
        "        one_hot_encoded_3 = torch.nn.functional.one_hot(sample[2].long(), self.num_classes_list[2])\n",
        "        # print(\"1 :\",one_hot_encoded_1.shape)\n",
        "        # print(\"2 :\",one_hot_encoded_2.shape)\n",
        "        # print(\"3 :\",one_hot_encoded_3.shape)\n",
        "        # print(\"4 :\",sample[3:].shape)\n",
        "        # Concatenate one-hot encoding with the remaining values\n",
        "        modified_sample = torch.cat((one_hot_encoded_1.float(), one_hot_encoded_2.float(), one_hot_encoded_3.float(), sample[3:]))\n",
        "\n",
        "        return modified_sample\n",
        "\n",
        "data = np.array(train_dataset[:10000])\n",
        "dataset = Dataset(data,[23030, 14372, 7])\n",
        "\n",
        "batch_size = 64\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVrP_kpedyb1",
        "outputId": "17beae9d-178c-4575-efc4-2330a230496b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37428"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "99r4Yc1VZwbI"
      },
      "outputs": [],
      "source": [
        "def differentiable_argmax(gumbel_softmax_sample):\n",
        "    _, max_indices = gumbel_softmax_sample.max(dim=-1, keepdim=True)\n",
        "    one_hot = torch.zeros_like(gumbel_softmax_sample).scatter_(-1, max_indices, 1.0)\n",
        "    return one_hot - gumbel_softmax_sample.detach() + gumbel_softmax_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fPfL8QW8uaqN"
      },
      "outputs": [],
      "source": [
        "def convert_generator_output(output_tensors):\n",
        "    sport_dist, dport_dist, proto_dist, cont = output_tensors\n",
        "    num_samples = sport_dist.size(0)\n",
        "    final = torch.zeros((num_samples, 22))\n",
        "\n",
        "    for sample_index in range(num_samples):\n",
        "        sport = torch.argmax(sport_dist[sample_index]) + 1\n",
        "        dport = torch.argmax(dport_dist[sample_index]) + 1\n",
        "        proto = torch.argmax(proto_dist[sample_index]) + 1\n",
        "\n",
        "        data_point = torch.cat((sport.view(1), dport.view(1), proto.view(1), cont[sample_index]))\n",
        "        final[sample_index] = data_point\n",
        "\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4Ee-uh20Eaih",
        "outputId": "60068658-63de-4999-c86d-67599f7dad72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/knel/virtual_envs/ankh-morpork/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            " 10%|         | 1/10 [00:16<02:27, 16.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], D Loss: 32.52910232543945, G Loss: 2.936279535293579\n",
            "\n",
            "Epoch 1 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|        | 2/10 [00:31<02:06, 15.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], D Loss: 32.10264587402344, G Loss: 2.907186508178711\n",
            "\n",
            "Epoch 2 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|       | 3/10 [00:47<01:50, 15.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], D Loss: 32.031349182128906, G Loss: 2.8964998722076416\n",
            "\n",
            "Epoch 3 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|      | 4/10 [01:02<01:33, 15.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], D Loss: 32.011932373046875, G Loss: 2.8933024406433105\n",
            "\n",
            "Epoch 4 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|     | 5/10 [01:18<01:18, 15.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/10], D Loss: 32.00856018066406, G Loss: 2.893223762512207\n",
            "\n",
            "Epoch 5 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|    | 6/10 [01:34<01:02, 15.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/10], D Loss: 31.993438720703125, G Loss: 2.890946626663208\n",
            "\n",
            "Epoch 6 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|   | 7/10 [01:49<00:46, 15.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/10], D Loss: 31.96874237060547, G Loss: 2.887634515762329\n",
            "\n",
            "Epoch 7 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|  | 8/10 [02:04<00:30, 15.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/10], D Loss: 31.961471557617188, G Loss: 2.88663387298584\n",
            "\n",
            "Epoch 8 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%| | 9/10 [02:20<00:15, 15.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/10], D Loss: 31.966236114501953, G Loss: 2.886303424835205\n",
            "\n",
            "Epoch 9 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 10/10 [02:36<00:00, 15.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/10], D Loss: 31.95956802368164, G Loss: 2.886603593826294\n",
            "\n",
            "Epoch 10 - Parameters of Generator:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Gumbel\n",
        "num_features=22\n",
        "# Generator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, num_features):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.hidden1 = nn.Linear(input_dim, 80)\n",
        "        self.hidden2 = nn.Linear(80, 80)\n",
        "\n",
        "        self.sport_hidden = nn.Linear(10, 23030)\n",
        "        self.dport_hidden = nn.Linear(10, 14372)\n",
        "        self.proto_hidden = nn.Linear(10, 7)\n",
        "\n",
        "        # self.sport_output = nn.Softmax(dim=1)\n",
        "        # self.dport_output = nn.Softmax(dim=1)\n",
        "        # self.proto_output = nn.Softmax(dim=1)\n",
        "\n",
        "        self.cont_output = nn.Linear(50, num_features - 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        # print(x.shape)\n",
        "        _sport_hidden = self.sport_hidden(x[:,:10])\n",
        "        _dport_hidden = self.dport_hidden(x[:,10:20])\n",
        "\n",
        "        _proto_hidden = self.proto_hidden(x[:,20:30])\n",
        "        # sport_output = self.sport_output(sport_hidden)\n",
        "        # dport_output = self.dport_output(dport_hidden)\n",
        "        # proto_output = self.proto_output(proto_hidden)\n",
        "\n",
        "        # Do not apply torch.argmax here, let the loss function handle it\n",
        "        cont_output = self.cont_output(x[:,30:])\n",
        "\n",
        "        # Concatenate the tensors along dimension 1\n",
        "        output_tensor = torch.cat((_sport_hidden, _dport_hidden, _proto_hidden, cont_output), dim=1)\n",
        "        return output_tensor\n",
        "        # return sport_output, dport_output, proto_output, cont_output\n",
        "# Discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=100, recurrent_dropout=0.4):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True, dropout=recurrent_dropout)\n",
        "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True, dropout=recurrent_dropout)\n",
        "\n",
        "        # Batch normalization layers\n",
        "        self.batch_norm1 = nn.BatchNorm1d(hidden_size * 2)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(hidden_size * 2)\n",
        "\n",
        "        # Linear output layer\n",
        "        self.output_layer = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM layers\n",
        "        out, _ = self.lstm1(x)\n",
        "        # print(out.shape)\n",
        "        out = self.batch_norm1(out)\n",
        "        # print(out.shape)\n",
        "        out, _ = self.lstm2(out)\n",
        "        # print(out.shape)\n",
        "        out = self.batch_norm2(out)\n",
        "        # print(out.shape)\n",
        "\n",
        "        # Global average pooling\n",
        "        # out = torch.mean(out, dim=1)\n",
        "        # print(out.shape)\n",
        "        # Output layer\n",
        "        # out = self.output_layer(out)\n",
        "        out = torch.sigmoid(self.output_layer(out))\n",
        "\n",
        "\n",
        "        return out  # Fix for the dimension mismatch\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 10\n",
        "data_dim = len(dataset[1])\n",
        "lr = 0.0002\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# Models\n",
        "generator = Generator(latent_dim, num_features)\n",
        "discriminator = Discriminator(data_dim)\n",
        "max_grad_norm = 1.0\n",
        "# Loss function and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "criterion_gen = nn.SmoothL1Loss()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "import csv\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "parameters_list = []\n",
        "csv_file_path = \"generator_parameters.csv\"\n",
        "# with open(csv_file_path, mode='w', newline='') as csv_file:\n",
        "#     fieldnames = [\"Epoch\", \"Parameter\", \"Value\"]\n",
        "#     csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "#     # Write the header\n",
        "#     csv_writer.writeheader()\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    total_loss_fake = 0\n",
        "    total_loss_real = 0\n",
        "    total_loss_gen = 0\n",
        "    for _ in data_loader:\n",
        "        # Train Discriminator\n",
        "        real_data = _\n",
        "        real_labels = torch.ones((batch_size, 1))\n",
        "        fake_labels = torch.zeros((batch_size, 1))\n",
        "\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # Real data\n",
        "        # print(\"Real data: \", real_data.shape)\n",
        "        output_real = discriminator(real_data)\n",
        "        # print(output_real)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "        # print(\"Loss:\", loss_real.item())\n",
        "        loss_real.backward()\n",
        "        # print(\"Gradients of a specific layer:\", generator.hidden1.weight.grad)\n",
        "        total_loss_real+=loss_real\n",
        "\n",
        "        # Fake data\n",
        "        noise = torch.randn(batch_size, latent_dim)\n",
        "        output = generator(noise)\n",
        "        # print(fake_data.shape)\n",
        "        sport_output = F.softmax(output[:, :23030], dim=1)\n",
        "        dport_output = F.softmax(output[:, 23030:23030+14372], dim=1)\n",
        "        proto_output = F.softmax(output[:, 23030+14372:23030+14372+7], dim=1)\n",
        "        cont_output = output[:, 23030+14372+7:]\n",
        "        # print(sport_output.shape)\n",
        "        # print(cont_output.shape)\n",
        "        fake_data = torch.cat((sport_output, dport_output, proto_output, cont_output), dim=1)\n",
        "        # fake_data = torch.cat((output[:,1].unsqueeze(1),output[:,2].unsqueeze(1),output[:,3].unsqueeze(1),cont_output), dim=1)\n",
        "        # print(fake_data.shape)\n",
        "\n",
        "        # print(\"shape \", fake_data.shape)\n",
        "        # print(fake_data)\n",
        "\n",
        "        # print(\"Fake data :\", len(_))\n",
        "        # for array in _:\n",
        "        #   print(\"Shape of array:\", array.shape)\n",
        "        # print(_[2])\n",
        "        # fake_data = convert_generator_output(_)\n",
        "        # print(fake_data.shape)\n",
        "        # print(fake_data)\n",
        "\n",
        "        output_fake = discriminator(fake_data)\n",
        "        # print(output_fake)\n",
        "        # print(fake_labels)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "        loss_fake.backward()\n",
        "        total_loss_fake+=loss_fake\n",
        "        # print(\"Gradients of a specific layer:\", generator.hidden1.weight.grad)\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        # generated_data = generator(noise)\n",
        "        output = generator(noise)\n",
        "        # print(fake_data.shape)\n",
        "        sport_output = F.softmax(output[:, :23030], dim=1)\n",
        "        dport_output = F.softmax(output[:, 23030:23030+14372], dim=1)\n",
        "        proto_output = F.softmax(output[:, 23030+14372:23030+14372+7], dim=1)\n",
        "        # sport_hidden = output[:, :23030]\n",
        "        # dport_hidden = output[:, 23030:23030+14372]\n",
        "        # proto_hidden = output[:, 23030+14372:23030+14372+7]\n",
        "        cont_output = output[:, 23030+14372+7:]\n",
        "        # temperature = 1.0  # You can adjust the temperature hyperparameter\n",
        "        # gumbel_dist = Gumbel(0, 1).sample(sport_hidden.shape)\n",
        "        # sport_output = F.softmax((sport_hidden + gumbel_dist) / temperature, dim=1)\n",
        "\n",
        "        # gumbel_dist = Gumbel(0, 1).sample(dport_hidden.shape)\n",
        "        # dport_output = F.softmax((dport_hidden + gumbel_dist) / temperature, dim=1)\n",
        "\n",
        "        # gumbel_dist = Gumbel(0, 1).sample(proto_hidden.shape)\n",
        "        # proto_output = F.softmax((proto_hidden + gumbel_dist) / temperature, dim=1)\n",
        "        # print(proto_output.shape)\n",
        "        # print(proto_output)\n",
        "        # print(differentiable_argmax(proto_output))\n",
        "        # print(differentiable_argmax(proto_output)[:,1])\n",
        "\n",
        "        # print(cont_output.shape)\n",
        "\n",
        "        # generated_data = torch.cat((differentiable_argmax(sport_output).unsqueeze(1),differentiable_argmax(dport_output)[:,1].unsqueeze(1),differentiable_argmax(proto_output)[:,1].unsqueeze(1), cont_output), dim=1)\n",
        "        # generated_data = torch.cat((sport_output[:,1].unsqueeze(1),dport_output[:,2].unsqueeze(1),proto_output[:,3].unsqueeze(1),cont_output), dim=1)\n",
        "        generated_data = torch.cat((sport_output, dport_output, proto_output, cont_output), dim=1)\n",
        "        # print(generated_data.shape)\n",
        "\n",
        "\n",
        "        # generated_data = convert_generator_output(generator(noise))\n",
        "        # print(generated_data.shape)\n",
        "        output_generated = discriminator(generated_data)\n",
        "        # loss_gen = criterion(output_generated, real_labels)\n",
        "        loss_gen = criterion_gen(output_generated, real_labels.float())\n",
        "        # print(\"Discriminator Output:\", output_generated)\n",
        "        # print(\"Real Labels:\", real_labels)\n",
        "        # print(\"Loss:\", loss_gen.item())\n",
        "        loss_gen.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_([generator.sport_hidden.weight], max_grad_norm)\n",
        "        total_loss_gen+=loss_gen\n",
        "        # for param in generator.sport_hidden.parameters():\n",
        "        #   print(param.requires_grad)\n",
        "        # for name, param in generator.named_parameters():\n",
        "        #   if name == 'sport_hidden.weight':\n",
        "        #       print(name, param.requires_grad)\n",
        "        #       print(name, param.grad)\n",
        "        #       print(f\"{name} - Gradient Statistics: Mean={param.grad.mean()}, Std={param.grad.std()}, Min={param.grad.min()}, Max={param.grad.max()}\")\n",
        "\n",
        "        # asdasdasd\n",
        "        # print(\"Gradients of a specific layer:\", generator.sport_hidden.weight.grad)\n",
        "        optimizer_g.step()\n",
        "\n",
        "    # Print losses at the end of each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {total_loss_real + total_loss_fake}, G Loss: {total_loss_gen}\")\n",
        "    print(f\"\\nEpoch {epoch + 1} - Parameters of Generator:\\n\")\n",
        "    epoch_parameters = {\"Epoch\": epoch + 1}\n",
        "    for name, param in generator.named_parameters():\n",
        "        # print(name, param.data)\n",
        "        # Store parameters in the dictionary\n",
        "        # epoch_parameters[name] = param.data\n",
        "        epoch_parameters[name] = copy.deepcopy(param.data)\n",
        "\n",
        "    # Append epoch parameters to the dictionary\n",
        "    parameters_list.append(epoch_parameters)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fxk8GIpLmreu",
        "outputId": "ed0ff371-daf9-47b2-9c6c-6c9a7826c92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias']\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n",
            "dict_keys(['Epoch', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias', 'sport_hidden.weight', 'sport_hidden.bias', 'dport_hidden.weight', 'dport_hidden.bias', 'proto_hidden.weight', 'proto_hidden.bias', 'cont_output.weight', 'cont_output.bias'])\n"
          ]
        }
      ],
      "source": [
        "csv_file_path = \"generator_parameters_1.csv\"\n",
        "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
        "    fieldnames = [\"Epoch\"] + list(generator.state_dict().keys())\n",
        "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    print(fieldnames)\n",
        "    # Write the header\n",
        "    csv_writer.writeheader()\n",
        "\n",
        "    # Write parameters for each epoch\n",
        "    for i in parameters_list:\n",
        "      # print(i)\n",
        "      # for key in i.keys():\n",
        "      # print(i.values())\n",
        "      print(i.keys())\n",
        "      row = {}\n",
        "      for key in i.keys():\n",
        "        row[key] = i[key]\n",
        "      print(row.keys())\n",
        "      csv_writer.writerow(i)\n",
        "\n",
        "      # exit\n",
        "      # csv_writer.writerows(i.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Z3rHrTIx7r",
        "outputId": "69f677af-399f-423a-d916-8573bbb74a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.0939,  0.0259,  0.0789, -0.1096, -0.0247,  0.1134,  0.0200, -0.0971,\n",
            "         0.0724, -0.0288,  0.1037, -0.0800,  0.0062,  0.0384,  0.0834, -0.0506,\n",
            "         0.0043, -0.1062, -0.0500])\n"
          ]
        }
      ],
      "source": [
        "print(parameters_list[1]['cont_output.bias'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmJ6zusRKPJm",
        "outputId": "03d958af-eb4a-45ac-b6a4-2744fce1bbae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.0943,  0.0255,  0.0768, -0.1122, -0.0260,  0.1110,  0.0208, -0.0991,\n",
            "         0.0731, -0.0295,  0.1050, -0.0800,  0.0076,  0.0349,  0.0833, -0.0497,\n",
            "         0.0037, -0.1050, -0.0500])\n"
          ]
        }
      ],
      "source": [
        "print(parameters_list[0]['cont_output.bias'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "7T8lzE4HoNH3",
        "outputId": "b84abe31-867f-43c3-e8fe-a09b24d360f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 80)\n",
            "(None, 80)\n",
            "(None, 23030)\n",
            "(None, 14372)\n",
            "(None, 7)\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "Random Input Shape: (1, 100)\n",
            "Hidden Layer Output Shape: (1, 23030)\n",
            "Sport Output Shape: (1, 14372)\n",
            "Dport Output Shape: (1, 7)\n",
            "Proto Output Shape: (1, 27)\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-496fd689432f>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dport Output Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Proto Output Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Continuous Output Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "}import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Assuming num_features is defined somewhere\n",
        "num_features = 30\n",
        "\n",
        "# Assuming wasserstein_loss is defined somewhere\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    # Placeholder for wasserstein_loss function\n",
        "    return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "def make_generator_model(input_dim):\n",
        "    input = tf.keras.layers.Input(shape=input_dim, name=\"generator input\")\n",
        "    hidden = tf.keras.layers.Dense(80, activation=\"relu\")(input)\n",
        "    hidden = tf.keras.layers.Dense(80, activation=\"relu\")(hidden)\n",
        "    print(hidden.shape)\n",
        "    sport_hidden = tf.keras.layers.Dense(23030, name=\"sport_hidden\")(hidden)\n",
        "    dport_hidden = tf.keras.layers.Dense(14372, name=\"dport_hidden\")(hidden)\n",
        "    proto_hidden = tf.keras.layers.Dense(7, name=\"proto_hidden\")(hidden)\n",
        "    print(hidden.shape)\n",
        "    print(sport_hidden.shape)\n",
        "    print(dport_hidden.shape)\n",
        "    print(proto_hidden.shape)\n",
        "    sport_output = tf.keras.layers.Softmax(1, name=\"sport_output\")(sport_hidden)\n",
        "    dport_output = tf.keras.layers.Softmax(1, name=\"dport_output\")(dport_hidden)\n",
        "    proto_output = tf.keras.layers.Softmax(1, name=\"proto_output\")(proto_hidden)\n",
        "    cont_output = tf.keras.layers.Dense(num_features-3, name=\"cont_output\")(hidden)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input, outputs=[sport_output, dport_output, proto_output, cont_output],\n",
        "                           name=\"Generator\")\n",
        "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
        "    # model.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming input_dim is 100 for example\n",
        "input_dim = 100\n",
        "\n",
        "# Create a random input tensor for testing\n",
        "random_input = np.random.rand(1, input_dim)\n",
        "\n",
        "# Instantiate the generator model\n",
        "generator_model = make_generator_model(input_dim)\n",
        "\n",
        "# Forward pass through the generator\n",
        "output_tensors = generator_model.predict(random_input)\n",
        "\n",
        "# Display the shapes after each step\n",
        "print(\"Random Input Shape:\", random_input.shape)\n",
        "print(\"Hidden Layer Output Shape:\", output_tensors[0].shape)\n",
        "print(\"Sport Output Shape:\", output_tensors[1].shape)\n",
        "print(\"Dport Output Shape:\", output_tensors[2].shape)\n",
        "print(\"Proto Output Shape:\", output_tensors[3].shape)\n",
        "print(\"Continuous Output Shape:\", output_tensors[4].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCpmnS0K_ywD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ankh-morpork",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
