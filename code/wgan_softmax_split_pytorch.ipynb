{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XpnS3F54NaTT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-11 20:44:29.626056: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-03-11 20:44:29.649947: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-11 20:44:29.649973: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-11 20:44:29.649990: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-11 20:44:29.655080: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-11 20:44:30.395886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ir3cCqLKNaTT"
      },
      "outputs": [],
      "source": [
        "attacks = pd.read_csv(\"/home/knel/virtual_envs/ankh-morpork/ICS_data_generation/data/wustl_attacks.csv\", sep=\",\", skiprows=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PlGx2v1BNaTT"
      },
      "outputs": [],
      "source": [
        "attacks = preprocessing.normalize(attacks.to_numpy()[:, 1:], norm=\"max\", axis=0)\n",
        "\n",
        "# # swap sport to front\n",
        "# attacks[:, [0, 1]] = attacks[:, [1, 0]]\n",
        "# # swap dport to after sport\n",
        "# attacks[:, [1, 2]] = attacks[:, [2, 1]]\n",
        "# # swap protocols to after dport\n",
        "# attacks[:, [2, 14]] = attacks[:, [14, 2]]\n",
        "\n",
        "# column order now -> sport, dport, protocols, continuous (discrete, discrete, discrete, continuous)\n",
        "\n",
        "\n",
        "\n",
        "# should probably add batch and shuffle\n",
        "\n",
        "train_dataset = attacks[:int(np.floor(attacks.shape[0]*3/4))]\n",
        "test_dataset = attacks[-5000:]\n",
        "\n",
        "num_features = attacks[:int(np.floor(attacks.shape[0]*3/4))].shape[1]\n",
        "seq_length = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifqu0pgzNaTT",
        "outputId": "2ec21186-565b-4bc1-e717-e15a2ba5dea6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(65261, 5000)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRR0cUA17sWo",
        "outputId": "7f31e18a-f1c4-4b54-de4a-1f4173faa25e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2.71767327e-01, 7.66154879e-03, 8.27653359e-03, 8.00000000e-01,\n",
              "       7.60913096e-03, 3.76573184e-03, 8.50432283e-03, 6.94274362e-05,\n",
              "       1.53165877e-05, 1.53165877e-05, 1.53165870e-05, 1.53165870e-05,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.77274300e-02,\n",
              "       8.43319240e-03, 9.59523343e-01, 9.99938846e-01, 9.59523343e-01,\n",
              "       9.59523343e-01, 9.59523343e-01, 3.33333333e-01, 3.85114806e-02,\n",
              "       2.97226418e-05, 9.59523343e-01, 2.77274300e-02])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTZSt9lp71rK",
        "outputId": "ae479dd0-cd5f-4cfd-cc3f-2b345b0c7a70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4.75905637e-01, 7.66154879e-03, 8.27653359e-03, 4.00000000e-01,\n",
              "       4.50540649e-03, 2.22970964e-03, 4.99717308e-03, 4.07958307e-05,\n",
              "       1.50599124e-05, 1.50599124e-05, 1.51733500e-05, 1.51733500e-05,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.83647165e-02,\n",
              "       7.88910350e-03, 5.68234365e-01, 9.99939177e-01, 5.68234365e-01,\n",
              "       5.68234365e-01, 5.68234365e-01, 8.90196078e-01, 2.28028504e-02,\n",
              "       1.75989326e-05, 5.68234365e-01, 1.83647165e-02])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5FZs594sNaTU"
      },
      "outputs": [],
      "source": [
        "# custom loss\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.keras.backend.mean(y_true * y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IPAYKjg2NaTU"
      },
      "outputs": [],
      "source": [
        "# clip model weights to a given hypercube\n",
        "class ClipConstraint(tf.keras.constraints.Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        "\n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn tf.keras.backend.clip(weights, -self.clip_value, self.clip_value)\n",
        "\n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HRvc7NspNaTV"
      },
      "outputs": [],
      "source": [
        "# Wasserstein loss for critic\n",
        "def critic_loss(pred_real, pred_fake):\n",
        "    return tf.keras.backend.mean(pred_real * pred_fake)\n",
        "\n",
        "# Wasserstein loss for generator\n",
        "def generator_loss(pred_fake):\n",
        "    return -tf.keras.backend.mean(pred_fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XgCI_3rzNaTV"
      },
      "outputs": [],
      "source": [
        "generator_optimiser = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.00005)\n",
        "critic_optimiser = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.00005)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xlMvaNXAMG9F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, _data, num_classes_list):\n",
        "        self.data = torch.tensor(_data, dtype=torch.float32)\n",
        "        self.num_classes_list = num_classes_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # One-hot encode each of the first three values with different num_classes\n",
        "        one_hot_encoded_1 = torch.nn.functional.one_hot(sample[0].long(), self.num_classes_list[0])\n",
        "        one_hot_encoded_2 = torch.nn.functional.one_hot(sample[1].long(), self.num_classes_list[1])\n",
        "        one_hot_encoded_3 = torch.nn.functional.one_hot(sample[2].long(), self.num_classes_list[2])\n",
        "        # print(\"1 :\",one_hot_encoded_1.shape)\n",
        "        # print(\"2 :\",one_hot_encoded_2.shape)\n",
        "        # print(\"3 :\",one_hot_encoded_3.shape)\n",
        "        # print(\"4 :\",sample[3:].shape)\n",
        "        # Concatenate one-hot encoding with the remaining values\n",
        "        modified_sample = torch.cat((one_hot_encoded_1.float(), one_hot_encoded_2.float(), one_hot_encoded_3.float(), sample[3:]))\n",
        "\n",
        "        return modified_sample\n",
        "\n",
        "data = np.array(train_dataset[:10000])\n",
        "\n",
        "# SWaT\n",
        "# dataset = Dataset(data,[23030, 14372, 7])\n",
        "\n",
        "# WUSTL\n",
        "dataset = Dataset(data,[44536, 7756, 4])\n",
        "\n",
        "batch_size = 64\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "99r4Yc1VZwbI"
      },
      "outputs": [],
      "source": [
        "def differentiable_argmax(gumbel_softmax_sample):\n",
        "    _, max_indices = gumbel_softmax_sample.max(dim=-1, keepdim=True)\n",
        "    one_hot = torch.zeros_like(gumbel_softmax_sample).scatter_(-1, max_indices, 1.0)\n",
        "    return one_hot - gumbel_softmax_sample.detach() + gumbel_softmax_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fPfL8QW8uaqN"
      },
      "outputs": [],
      "source": [
        "def convert_generator_output(output_tensors):\n",
        "    sport_dist, dport_dist, proto_dist, cont = output_tensors\n",
        "    num_samples = sport_dist.size(0)\n",
        "    final = torch.zeros((num_samples, 22))\n",
        "\n",
        "    for sample_index in range(num_samples):\n",
        "        sport = torch.argmax(sport_dist[sample_index]) + 1\n",
        "        dport = torch.argmax(dport_dist[sample_index]) + 1\n",
        "        proto = torch.argmax(proto_dist[sample_index]) + 1\n",
        "\n",
        "        data_point = torch.cat((sport.view(1), dport.view(1), proto.view(1), cont[sample_index]))\n",
        "        final[sample_index] = data_point\n",
        "\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4Ee-uh20Eaih",
        "outputId": "60068658-63de-4999-c86d-67599f7dad72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/knel/virtual_envs/ankh-morpork/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            " 20%|██        | 1/5 [02:00<08:03, 120.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], D Loss: 39.376251220703125, G Loss: 19.626096725463867\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [03:43<05:29, 109.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5], D Loss: 39.128684997558594, G Loss: 19.535879135131836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [05:13<03:22, 101.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5], D Loss: 39.08113479614258, G Loss: 19.52677345275879\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [06:45<01:37, 97.47s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5], D Loss: nan, G Loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [08:22<00:00, 100.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5], D Loss: nan, G Loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Gumbel\n",
        "num_features=22\n",
        "# Generator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, num_features):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.hidden1 = nn.Linear(input_dim, 8000)\n",
        "        self.hidden2 = nn.Linear(8000, 80)\n",
        "\n",
        "        # SWaT\n",
        "        # self.sport_hidden = nn.Linear(10, 23030)\n",
        "        # self.dport_hidden = nn.Linear(10, 14372)\n",
        "        # self.proto_hidden = nn.Linear(10, 7)\n",
        "\n",
        "        # WUSTL\n",
        "        self.sport_hidden = nn.Linear(10, 44536)\n",
        "        self.dport_hidden = nn.Linear(10, 7756)\n",
        "        self.proto_hidden = nn.Linear(10, 4)\n",
        "\n",
        "        # self.sport_output = nn.Softmax(dim=1)\n",
        "        # self.dport_output = nn.Softmax(dim=1)\n",
        "        # self.proto_output = nn.Softmax(dim=1)\n",
        "\n",
        "        self.cont_output = nn.Linear(50, num_features - 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        # r = nn.LeakyReLU()\n",
        "        # x = r(self.hidden1(x))\n",
        "        # x = r(self.hidden2(x))\n",
        "        # print(x.shape)\n",
        "\n",
        "        _sport_hidden = self.sport_hidden(x[:,:10])\n",
        "        _dport_hidden = self.dport_hidden(x[:,10:20])\n",
        "\n",
        "        _proto_hidden = self.proto_hidden(x[:,20:30])\n",
        "\n",
        "\n",
        "        # sport_output = self.sport_output(sport_hidden)\n",
        "        # dport_output = self.dport_output(dport_hidden)\n",
        "        # proto_output = self.proto_output(proto_hidden)\n",
        "\n",
        "        # Do not apply torch.argmax here, let the loss function handle it\n",
        "        cont_output = self.cont_output(x[:,30:])\n",
        "        # cont_output = self.cont_output(x)\n",
        "\n",
        "        # Concatenate the tensors along dimension 1\n",
        "        output_tensor = torch.cat((_sport_hidden, _dport_hidden, _proto_hidden, cont_output), dim=1)\n",
        "        return output_tensor\n",
        "        # return sport_output, dport_output, proto_output, cont_output\n",
        "# Discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=100, recurrent_dropout=0.4):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True, dropout=recurrent_dropout)\n",
        "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True, dropout=recurrent_dropout)\n",
        "\n",
        "        # Batch normalization layers\n",
        "        self.batch_norm1 = nn.BatchNorm1d(hidden_size * 2)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(hidden_size * 2)\n",
        "\n",
        "        # Linear output layer\n",
        "        self.output_layer = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        # LSTM layers\n",
        "        out, _ = self.lstm1(x)\n",
        "        # print(out.shape)\n",
        "        out = self.batch_norm1(out)\n",
        "        # print(out.shape)\n",
        "        out, _ = self.lstm2(out)\n",
        "        # print(out.shape)\n",
        "        out = self.batch_norm2(out)\n",
        "        # print(out.shape)\n",
        "\n",
        "        # Global average pooling\n",
        "        # out = torch.mean(out, dim=1)\n",
        "        # print(out.shape)\n",
        "        # Output layer\n",
        "        # out = self.output_layer(out)\n",
        "        out = torch.sigmoid(self.output_layer(out))\n",
        "\n",
        "\n",
        "        return out  # Fix for the dimension mismatch\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "data_dim = len(dataset[1])\n",
        "lr = 0.0002\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "discriminator_loops = 1\n",
        "\n",
        "# Models\n",
        "generator = Generator(latent_dim, num_features)\n",
        "discriminator = Discriminator(data_dim)\n",
        "max_grad_norm = 1.0\n",
        "# Loss function and optimizers\n",
        "# criterion = nn.BCELoss()\n",
        "criterion = nn.HuberLoss()\n",
        "# criterion_gen = nn.SmoothL1Loss()\n",
        "criterion_gen = nn.HuberLoss()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "import csv\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "parameters_list = []\n",
        "disc_parameters_list = []\n",
        "dlosses = []\n",
        "glosses = []\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    total_loss_fake = 0\n",
        "    total_loss_real = 0\n",
        "    total_loss_gen = 0\n",
        "    for batch in data_loader:\n",
        "        # Train Discriminator\n",
        "        for _ in range(discriminator_loops):\n",
        "            real_data = batch\n",
        "            real_labels = torch.ones((batch_size, 1))\n",
        "            fake_labels = torch.zeros((batch_size, 1))\n",
        "\n",
        "            optimizer_d.zero_grad()\n",
        "\n",
        "            # Real data\n",
        "            # print(\"Real data: \", real_data.shape)\n",
        "            output_real = discriminator(real_data)\n",
        "            # print(output_real)\n",
        "            loss_real = criterion(output_real, real_labels)\n",
        "            # print(\"Loss:\", loss_real.item())\n",
        "            loss_real.backward()\n",
        "            # print(\"Gradients of a specific layer:\", generator.hidden1.weight.grad)\n",
        "            total_loss_real+=loss_real\n",
        "\n",
        "            # Fake data\n",
        "            noise = torch.randn(batch_size, latent_dim)\n",
        "            output = generator(noise)\n",
        "\n",
        "            # SWaT\n",
        "            # sport_output = F.softmax(output[:, :23030], dim=1)\n",
        "            # dport_output = F.softmax(output[:, 23030:23030+14372], dim=1)\n",
        "            # proto_output = F.softmax(output[:, 23030+14372:23030+14372+7], dim=1)\n",
        "            # cont_output = output[:, 23030+14372+7:]\n",
        "\n",
        "            # WUSTL\n",
        "            sport_output = F.softmax(output[:, :44536], dim=1)\n",
        "            dport_output = F.softmax(output[:, 44536:44536+7756], dim=1)\n",
        "            proto_output = F.softmax(output[:, 44536+7756:44536+7756+4], dim=1)\n",
        "            cont_output = output[:, 44536+7756+4:]\n",
        "\n",
        "            fake_data = torch.cat((sport_output, dport_output, proto_output, cont_output), dim=1)\n",
        "\n",
        "            output_fake = discriminator(fake_data)\n",
        "            # print(output_fake)\n",
        "            # print(fake_labels)\n",
        "            loss_fake = criterion(output_fake, fake_labels)\n",
        "            loss_fake.backward()\n",
        "            total_loss_fake+=loss_fake\n",
        "            # print(\"Gradients of a specific layer:\", generator.hidden1.weight.grad)\n",
        "            optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        # generated_data = generator(noise)\n",
        "        output = generator(noise)\n",
        "        # print(fake_data.shape)\n",
        "\n",
        "        # SWaT\n",
        "        # sport_output = F.softmax(output[:, :23030], dim=1)\n",
        "        # dport_output = F.softmax(output[:, 23030:23030+14372], dim=1)\n",
        "        # proto_output = F.softmax(output[:, 23030+14372:23030+14372+7], dim=1)\n",
        "        # cont_output = output[:, 23030+14372+7:]\n",
        "\n",
        "        # WUSTL\n",
        "        sport_output = F.softmax(output[:, :44536], dim=1)\n",
        "        dport_output = F.softmax(output[:, 44536:44536+7756], dim=1)\n",
        "        proto_output = F.softmax(output[:, 44536+7756:44536+7756+4], dim=1)\n",
        "        cont_output = output[:, 44536+7756+4:]\n",
        "\n",
        "\n",
        "        # temperature = 1.0  # You can adjust the temperature hyperparameter\n",
        "        # gumbel_dist = Gumbel(0, 1).sample(sport_hidden.shape)\n",
        "        # sport_output = F.softmax((sport_hidden + gumbel_dist) / temperature, dim=1)\n",
        "\n",
        "        # gumbel_dist = Gumbel(0, 1).sample(dport_hidden.shape)\n",
        "        # dport_output = F.softmax((dport_hidden + gumbel_dist) / temperature, dim=1)\n",
        "\n",
        "        # gumbel_dist = Gumbel(0, 1).sample(proto_hidden.shape)\n",
        "        # proto_output = F.softmax((proto_hidden + gumbel_dist) / temperature, dim=1)\n",
        "\n",
        "        # generated_data = torch.cat((differentiable_argmax(sport_output).unsqueeze(1),differentiable_argmax(dport_output)[:,1].unsqueeze(1),differentiable_argmax(proto_output)[:,1].unsqueeze(1), cont_output), dim=1)\n",
        "        # generated_data = torch.cat((sport_output[:,1].unsqueeze(1),dport_output[:,2].unsqueeze(1),proto_output[:,3].unsqueeze(1),cont_output), dim=1)\n",
        "        generated_data = torch.cat((sport_output, dport_output, proto_output, cont_output), dim=1)\n",
        "        # print(generated_data.shape)\n",
        "\n",
        "\n",
        "        # generated_data = convert_generator_output(generator(noise))\n",
        "        # print(generated_data.shape)\n",
        "        output_generated = discriminator(generated_data)\n",
        "        # loss_gen = criterion(output_generated, real_labels)\n",
        "        loss_gen = criterion_gen(output_generated, real_labels.float())\n",
        "        # print(\"Discriminator Output:\", output_generated)\n",
        "        # print(\"Real Labels:\", real_labels)\n",
        "        # print(\"Loss:\", loss_gen.item())\n",
        "        loss_gen.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_([generator.sport_hidden.weight], max_grad_norm)\n",
        "        total_loss_gen+=loss_gen\n",
        "        # for param in generator.sport_hidden.parameters():\n",
        "        #   print(param.requires_grad)\n",
        "        # for name, param in generator.named_parameters():\n",
        "        #   if name == 'sport_hidden.weight':\n",
        "        #       print(name, param.requires_grad)\n",
        "        #       print(name, param.grad)\n",
        "        #       print(f\"{name} - Gradient Statistics: Mean={param.grad.mean()}, Std={param.grad.std()}, Min={param.grad.min()}, Max={param.grad.max()}\")\n",
        "\n",
        "        # asdasdasd\n",
        "        # print(\"Gradients of a specific layer:\", generator.sport_hidden.weight.grad)\n",
        "        optimizer_g.step()\n",
        "\n",
        "    # Print losses at the end of each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {total_loss_real + total_loss_fake}, G Loss: {total_loss_gen}\")\n",
        "    # print(f\"\\nEpoch {epoch + 1} - Parameters of Generator:\\n\")\n",
        "    epoch_parameters = {\"Epoch\": epoch + 1}\n",
        "    # epoch_parameters = {}\n",
        "    for name, param in generator.named_parameters():\n",
        "        # print(name, param.data)\n",
        "        # Store parameters in the dictionary\n",
        "        # epoch_parameters[\"Epoch\"] = epoch + 1\n",
        "        # epoch_parameters[name] = param.data\n",
        "        epoch_parameters[name] = copy.deepcopy(param.data.numpy())\n",
        "\n",
        "    disc_params = {\"Epoch\": epoch + 1}\n",
        "    for name, param in discriminator.named_parameters():\n",
        "        disc_params[name] = copy.deepcopy(param.data.numpy())\n",
        "\n",
        "    # Append epoch parameters to the dictionary\n",
        "    parameters_list.append(epoch_parameters)\n",
        "    disc_parameters_list.append(disc_params)\n",
        "    dlosses.append(loss_real + loss_fake)\n",
        "    glosses.append(loss_gen)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import csv\n",
        "\n",
        "# with open('params.csv','w') as f:\n",
        "#     w = csv.writer(f)\n",
        "#     for epoch in parameters_list:\n",
        "#         w.writerows(epoch.items())\n",
        "\n",
        "# with open('disc_params.csv', 'w') as f:\n",
        "#     w = csv.writer(f)\n",
        "#     for epoch in disc_parameters_list:\n",
        "#         w.writerows(epoch.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3331224210>]"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dlosses = [dloss.detach().numpy() for dloss in dlosses]\n",
        "glosses = [gloss.detach().numpy() for gloss in glosses]\n",
        "\n",
        "# plt.figure(1)\n",
        "plt.plot(dlosses)\n",
        "# plt.figure(2)\n",
        "plt.plot(glosses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fxk8GIpLmreu",
        "outputId": "ed0ff371-daf9-47b2-9c6c-6c9a7826c92b"
      },
      "outputs": [],
      "source": [
        "# csv_file_path = \"generator_parameters_1.csv\"\n",
        "# with open(csv_file_path, mode='w', newline='') as csv_file:\n",
        "#     fieldnames = [\"Epoch\"] + list(generator.state_dict().keys())\n",
        "#     csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "#     print(fieldnames)\n",
        "#     # Write the header\n",
        "#     csv_writer.writeheader()\n",
        "\n",
        "#     # Write parameters for each epoch\n",
        "#     for i in parameters_list:\n",
        "#       # print(i)\n",
        "#       # for key in i.keys():\n",
        "#       # print(i.values())\n",
        "#       print(i.keys())\n",
        "#       row = {}\n",
        "#       for key in i.keys():\n",
        "#         row[key] = i[key]\n",
        "#       print(row.keys())\n",
        "#       csv_writer.writerow(i)\n",
        "\n",
        "#       # exit\n",
        "#       # csv_writer.writerows(i.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_softmax_output_swat(tens):\n",
        "\n",
        "    output = np.asarray([var.detach().numpy() for var in tens])\n",
        "\n",
        "    sport = output[:, :23030]\n",
        "    dport = output[:, 23030:23030+14372]\n",
        "    proto = output[:, 23030+14372:23030+14372+7]\n",
        "    cont = output[:, 23030+14372+7:]\n",
        "\n",
        "    num_samples = sport.shape[0]\n",
        "\n",
        "    sport_select = np.argmax(sport, axis=1)\n",
        "    dport_select = np.argmax(dport, axis=1)\n",
        "    proto_select = np.argmax(proto, axis=1)\n",
        "\n",
        "    return np.hstack((sport_select.reshape(num_samples, 1), dport_select.reshape(num_samples, 1), proto_select.reshape(num_samples, 1), cont))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_softmax_output_wustl(tens):\n",
        "\n",
        "    output = np.asarray([var.detach().numpy() for var in tens])\n",
        "\n",
        "    sport = output[:, :44536]\n",
        "    dport = output[:, 44536:44536+7756]\n",
        "    proto = output[:, 44536+7756:44536+7756+4]\n",
        "    cont = output[:, 44536+7756+4:]\n",
        "\n",
        "    num_samples = sport.shape[0]\n",
        "\n",
        "    sport_select = np.argmax(sport, axis=1)\n",
        "    dport_select = np.argmax(dport, axis=1)\n",
        "    proto_select = np.argmax(proto, axis=1)\n",
        "\n",
        "    return np.hstack((sport_select.reshape(num_samples, 1), dport_select.reshape(num_samples, 1), proto_select.reshape(num_samples, 1), cont))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KstestResult(statistic=0.4811363636363636, pvalue=0.0, statistic_location=0.14285714285714285, statistic_sign=1)"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import scipy\n",
        "test_noise = torch.randn(test_dataset.shape[0], latent_dim)\n",
        "preds = generator(test_noise)\n",
        "data = convert_softmax_output_wustl(preds)\n",
        "scipy.stats.ks_2samp(test_dataset.reshape(preds.shape[0]*num_features,), data.reshape(preds.shape[0]*num_features,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PCpmnS0K_ywD"
      },
      "outputs": [],
      "source": [
        "# df = pd.DataFrame(data)\n",
        "# df.to_csv(\"gan_softmax_wustl_network.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ankh-morpork",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
