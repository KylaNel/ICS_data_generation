{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 15:51:55.330514: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-13 15:51:55.353492: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-13 15:51:55.353515: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-13 15:51:55.353531: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-13 15:51:55.357730: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-13 15:51:55.358036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-13 15:51:57.323015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = pd.read_csv(\"/home/knel/virtual_envs/ankh-morpork/ICS_data_generation/data/swat_processed.csv\", sep=\",\", usecols=range(1,23), skiprows=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attacks = preprocessing.normalize(attacks.to_numpy(), norm=\"max\", axis=0)\n",
    "attacks = attacks.to_numpy()\n",
    "\n",
    "# swap sport to front\n",
    "attacks[:, [0, 1]] = attacks[:, [1, 0]]\n",
    "# swap dport to after sport\n",
    "attacks[:, [1, 2]] = attacks[:, [2, 1]]\n",
    "# swap protocols to after dport\n",
    "attacks[:, [2, 14]] = attacks[:, [14, 2]]\n",
    "\n",
    "# column order now -> sport, dport, protocols, continuous (discrete, discrete, discrete, continuous)\n",
    "\n",
    "\n",
    "\n",
    "# should probably add batch and shuffle\n",
    "\n",
    "train_dataset = attacks[:int(np.floor(attacks.shape[0]*3/4))]\n",
    "test_dataset = attacks[int(np.floor(attacks.shape[0]*3/4)):]\n",
    "\n",
    "num_features = attacks[:int(np.floor(attacks.shape[0]*3/4))].shape[1]\n",
    "seq_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1993625, 664542)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip model weights to a given hypercube\n",
    "class ClipConstraint(tf.keras.constraints.Constraint):\n",
    "\t# set clip value when initialized\n",
    "\tdef __init__(self, clip_value):\n",
    "\t\tself.clip_value = clip_value\n",
    "\n",
    "\t# clip model weights to hypercube\n",
    "\tdef __call__(self, weights):\n",
    "\t\treturn tf.keras.backend.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "\t# get the config\n",
    "\tdef get_config(self):\n",
    "\t\treturn {'clip_value': self.clip_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise will have same dimension as one flow\n",
    "\n",
    "def make_generator_model(input_dim):\n",
    "    \"\"\"\n",
    "    The generator takes noise as input and goes through a couple\n",
    "    of hidden layers. Then, it splits the output into 3 softmax layers\n",
    "    (one for each categorical variable with the same dimension as the \n",
    "    number of possible values that variable can take) and a dense\n",
    "    layer for all the rest of the continuous variables.\n",
    "\n",
    "    Bigger node size in the initial dense layer decreases critic loss substantially.\n",
    "\n",
    "    Adding sigmoid acativation to continuous output layer drops generator loss\n",
    "    (probably also really need this so it doesn't predict negative values).\n",
    "    \"\"\"\n",
    "    input = tf.keras.layers.Input(shape=input_dim, name=\"generator input\")\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(100000)(input)\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(50000)(hidden)\n",
    "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "    hidden = tf.keras.layers.LeakyReLU(0.2)(hidden)\n",
    "\n",
    "    sport_hidden = tf.keras.layers.Dense(23030, name=\"sport_hidden\")(hidden)\n",
    "    sport_hidden = tf.keras.layers.BatchNormalization()(sport_hidden)\n",
    "    sport_hidden = tf.keras.layers.LeakyReLU(0.2)(sport_hidden)\n",
    "\n",
    "    dport_hidden = tf.keras.layers.Dense(14372, name=\"dport_hidden\")(hidden)\n",
    "    dport_hidden = tf.keras.layers.BatchNormalization()(dport_hidden)\n",
    "    dport_hidden = tf.keras.layers.LeakyReLU(0.2)(dport_hidden)\n",
    "\n",
    "    proto_hidden = tf.keras.layers.Dense(7, name=\"proto_hidden\")(hidden)\n",
    "    proto_hidden = tf.keras.layers.BatchNormalization()(proto_hidden)\n",
    "    proto_hidden = tf.keras.layers.LeakyReLU(0.2)(proto_hidden)\n",
    "\n",
    "    # dense output for continuous, softmax for categorical\n",
    "    sport_output = tf.keras.layers.Softmax(1, name=\"sport_output\")(sport_hidden)\n",
    "    dport_output = tf.keras.layers.Softmax(1, name=\"dport_output\")(dport_hidden)\n",
    "    proto_output = tf.keras.layers.Softmax(1, name=\"proto_output\")(proto_hidden)\n",
    "    cont_output = tf.keras.layers.Dense(num_features-3, name=\"cont_output\", activation=\"sigmoid\")(hidden)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=[sport_output, dport_output, proto_output, cont_output], \n",
    "                           name=\"Generator\")\n",
    "    # opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "    # model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_generator_output(arr):\n",
    "    \"\"\"\n",
    "    For each categorical feature, takes the most likely probability given\n",
    "    by the Softmax output and concatenates it with the continuous features.\n",
    "    This is so the generated data has the same shape as the real data.\n",
    "    \"\"\"\n",
    "    sport_dist, dport_dist, proto_dist, cont = arr\n",
    "    num_samples = sport_dist.shape[0]\n",
    "    final = np.zeros(22)\n",
    "    for sample_index in range(num_samples):\n",
    "        sport = np.array([np.argmax(sport_dist[sample_index])+1])\n",
    "        dport = np.array([np.argmax(dport_dist[sample_index])+1])\n",
    "        proto = np.array([np.argmax(proto_dist[sample_index])+1])\n",
    "        data_point = np.concatenate((sport, dport, proto, cont[sample_index]))\n",
    "        final = np.vstack((final, data_point))\n",
    "    return tf.convert_to_tensor(final[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_generator_output_tensor(tens):\n",
    "    sport_dist, dport_dist, proto_dist, cont = tens\n",
    "    sport_dist = sport_dist.numpy()\n",
    "    dport_dist = dport_dist.numpy()\n",
    "    proto_dist = proto_dist.numpy()\n",
    "    cont = cont.numpy()\n",
    "    # num_samples = 5\n",
    "    # print(tf.shape(sport_dist))\n",
    "    # final = tf.zeros(num_features,)\n",
    "    # for sample_index in range(num_samples):\n",
    "    #     sport_val = tf.constant(tf.argmax(sport_dist[sample_index]).numpy(), dtype=tf.float32)\n",
    "    #     dport_val = tf.constant(tf.argmax(dport_dist[sample_index]).numpy(), dtype=tf.float32)\n",
    "    #     proto_val = tf.constant(tf.argmax(proto_dist[sample_index]).numpy(), dtype=tf.float32)\n",
    "    #     disc = tf.stack([sport_val, dport_val, proto_val], 0)\n",
    "    #     data_point = tf.concat([disc, cont[sample_index]], 0)\n",
    "    #     final = tf.stack([final, data_point], 1)\n",
    "    # print(final)\n",
    "    num_samples = sport_dist.shape[0]\n",
    "    final = np.zeros(22)\n",
    "    for sample_index in range(num_samples):\n",
    "        sport = np.array([np.argmax(sport_dist[sample_index])+1])\n",
    "        dport = np.array([np.argmax(dport_dist[sample_index])+1])\n",
    "        proto = np.array([np.argmax(proto_dist[sample_index])+1])\n",
    "        data_point = np.concatenate((sport, dport, proto, cont[sample_index]))\n",
    "        final = np.vstack((final, data_point))\n",
    "    return tf.convert_to_tensor(final[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = make_generator_model(50)\n",
    "# noise = generate_latent_points(50, 5)\n",
    "# X_fake = generator(noise, training=False)\n",
    "# convert_generator_output_tensor(X_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_critic_model(input_shape):\n",
    "    \"\"\"\n",
    "    Critic (discriminator) more or less as described by the WGAN people.\n",
    "    The output layer uses linear activation since this is a critic and not \n",
    "    a standard discriminator.\n",
    "    \"\"\"\n",
    "    const = ClipConstraint(0.01)\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=input_shape, name=\"discriminator input\")\n",
    "    hidden = tf.keras.layers.LSTM(100, recurrent_dropout=0.4, return_sequences=True, kernel_constraint=const)(input)\n",
    "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "    hidden = tf.keras.layers.LSTM(100, recurrent_dropout=0.4, kernel_constraint=const)(hidden)\n",
    "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"linear\")(hidden)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output, name=\"Critic\")\n",
    "\n",
    "    # opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "    # model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gan(generator, critic, gen_input_dim):\n",
    "    \"\"\"\n",
    "    Would be a convenient way to train the GAN but this doesn't work\n",
    "    because the generator output has 4 tensors (one for each layer)\n",
    "    and the critic expects an input with the same dimension as the real data.\n",
    "\n",
    "    We can't just change the shape of the real data because I'd have to one-hot encode\n",
    "    the categorical variables - these datasets are huge and it would take FOREVER.\n",
    "    \"\"\"\n",
    "    for layer in critic.layers:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=gen_input_dim, name=\"combined input\")\n",
    "    x = generator(input)\n",
    "    x = critic(x)\n",
    "    model = tf.keras.Model(inputs=input, outputs=x)\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions that may or may not be useful\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t\"\"\"\n",
    "\tGenerate points in latent space as input for the generator.\n",
    "\t\"\"\"\n",
    "\t# generate points in the latent space\n",
    "\tx_input = np.random.randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t\"\"\"\n",
    "\tSelect real samples.\n",
    "\t\"\"\"\n",
    "\t# choose random instances\n",
    "\tix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "\t# select data\n",
    "\tX = dataset[ix]\n",
    "\t# refactor real data to have same shape as generated data (4 tensors)\n",
    "\t\n",
    "\t# generate class labels, -1 for 'real'\n",
    "\ty = -np.ones((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t\"\"\"\n",
    "\tUse the generator to generate n fake examples, with class labels\n",
    "\t\"\"\"\n",
    "\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\tX = convert_generator_output(X)\n",
    "\t# create class labels with 1.0 for 'fake'\n",
    "\ty = np.ones((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wasserstein loss for critic\n",
    "def critic_loss(pred_real, pred_fake):\n",
    "    return tf.keras.backend.mean(pred_real * pred_fake)\n",
    "\n",
    "# Wasserstein loss for generator\n",
    "def generator_loss(pred_fake):\n",
    "    return -tf.keras.backend.mean(pred_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimiser = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "critic_optimiser = tf.keras.optimizers.RMSprop(learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(generator, critic, dataset, latent_dim, n_batch=64, n_critic=5):\n",
    "\n",
    "    for _ in range(n_critic):\n",
    "\n",
    "        \"\"\"\n",
    "        Train critic more often than the generator for WGAN\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            # Critic training works fine\n",
    "            X_real, y_real = generate_real_samples(dataset, n_batch)\n",
    "            noise = generate_latent_points(latent_dim, n_batch)\n",
    "            X_fake = generator(noise, training=False)\n",
    "            X_fake = convert_generator_output(X_fake)\n",
    "\n",
    "            pred_real = critic(X_real, training=True)\n",
    "            pred_fake = critic(X_fake, training=True)\n",
    "\n",
    "            c_loss = critic_loss(pred_real, pred_fake)\n",
    "            print(f\"Critic loss: {c_loss}\")\n",
    "\n",
    "        critic_gradients = critic_tape.gradient(c_loss, critic.trainable_variables)\n",
    "        critic_optimiser.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "        \n",
    "    \"\"\" \n",
    "    Train generator\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        X_fake = generator(noise, training=True)\n",
    "        X_fake = convert_generator_output(X_fake)\n",
    "        pred_fake = critic(X_fake, training=False)\n",
    "        g_loss = generator_loss(pred_fake)\n",
    "        print(f\"Generator loss: {g_loss}\")\n",
    "        print(X_fake[0])\n",
    "    \n",
    "    # These gradients are all None and I can't figure out why\n",
    "    # Are the multiple output layers of the generator a problem here?\n",
    "    # Maybe the definition of the loss function?\n",
    "    generator_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    print(generator_gradients)\n",
    "    generator_optimiser.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "\n",
    "\n",
    "    return c_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, n_epochs=2, latent_dim=50):\n",
    "\n",
    "    generator = make_generator_model(latent_dim)\n",
    "    critic = make_critic_model((num_features, 1))\n",
    "\n",
    "    c_losses, g_losses = [], []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        print(f\"Epoch: {i+1}\")\n",
    "        for batch in dataset:\n",
    "            print(batch.shape)\n",
    "            c_loss, g_loss = train_step(generator, critic, batch, latent_dim)\n",
    "            c_losses.append(c_loss)\n",
    "            g_losses.append(g_loss)\n",
    "\n",
    "    return c_losses, g_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 15:58:37.750853: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 20000000000 exceeds 10% of free system memory.\n",
      "2024-02-13 15:58:39.403099: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 20000000000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Test training on a small sample of the dataset\n",
    "dataset = np.array_split(attacks, 3125)\n",
    "# dataset = (tf.data.Dataset.from_tensor_slices(attacks[:1000]).shuffle(10000).batch(64))\n",
    "c_losses, g_losses = train(dataset, 2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "\n",
    "class WGAN(tf.keras.models.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            critic: tf.keras.models.Model,\n",
    "            generator: tf.keras.models.Model,\n",
    "            latent_dim: int,\n",
    "            critic_extra_steps: int=5,\n",
    "    ) -> None:\n",
    "        super(WGAN, self).__init__()\n",
    "        self.critic = critic\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.critic_extra_steps = critic_extra_steps\n",
    "\n",
    "    def compile(\n",
    "            self,\n",
    "            critic_optimiser: tf.keras.optimizers.Optimizer,\n",
    "            generator_optimiser: tf.keras.optimizers.Optimizer,\n",
    "            critic_loss: typing.Callable,\n",
    "            generator_loss: typing.Callable,\n",
    "            **kwargs\n",
    "    ) -> None:\n",
    "        super(WGAN, self).compile(**kwargs)\n",
    "        self.critic_optimiser = critic_optimiser\n",
    "        self.generator_optimiser = generator_optimiser\n",
    "        self.critic_loss = critic_loss\n",
    "        self.generator_loss = generator_loss\n",
    "\n",
    "    def train_step(self, databatch):\n",
    "        n_batch = 64\n",
    "        noise = generate_latent_points(50, n_batch)\n",
    "        \n",
    "        for _ in range(self.critic_extra_steps):\n",
    "\n",
    "            with tf.GradientTape() as critic_tape:\n",
    "                # Critic training works fine\n",
    "                # X_real, y_real = generate_real_samples(dataset, n_batch)\n",
    "                # X_fake = self.generator(noise, training=False)\n",
    "                X_fake = self.generator.predict(noise)\n",
    "                print(X_fake)\n",
    "                # X_fake = convert_generator_output_tensor(X_fake)\n",
    "\n",
    "                # pred_real = self.critic(X_real, training=True)\n",
    "                pred_real = self.critic(databatch, training=True)\n",
    "                pred_fake = self.critic(X_fake, training=True)\n",
    "\n",
    "                c_loss = self.critic_loss(pred_real, pred_fake)\n",
    "\n",
    "            critic_gradients = critic_tape.gradient(c_loss, self.critic.trainable_variables)\n",
    "            critic_optimiser.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        \"\"\" \n",
    "        Train generator\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            X_fake = self.generator(noise, training=True)\n",
    "            X_fake = convert_generator_output(X_fake)\n",
    "            pred_fake = self.critic(X_fake, training=False)\n",
    "            g_loss = generator_loss(pred_fake)\n",
    "\n",
    "        # These gradients are all None and I can't figure out why\n",
    "        # Are the multiple output layers of the generator a problem here?\n",
    "        # Maybe the definition of the loss function?\n",
    "        generator_gradients = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        generator_optimiser.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
    "\n",
    "        return c_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = attacks[:100000]\n",
    "dataset = (tf.data.Dataset.from_tensor_slices(attacks[:1000]).shuffle(10000).batch(64))\n",
    "\n",
    "generator = make_generator_model(50)\n",
    "critic = make_critic_model((num_features, 1))\n",
    "\n",
    "gan = WGAN(critic, generator, 50, 5)\n",
    "gan.compile(critic_optimiser, generator_optimiser, critic_loss, generator_loss)\n",
    "gan.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = make_generator_model(50)\n",
    "# critic = make_critic_model((num_features, 1))\n",
    "# noise = generate_latent_points(50, 3)\n",
    "# X_fake = generator(noise, training=False)\n",
    "# X_fake = convert_generator_output(X_fake)\n",
    "# y_fake = critic(X_fake)\n",
    "# g_loss = generator_loss(y_fake)\n",
    "\n",
    "# generator.compile(loss=generator_loss, optimizer=generator_optimiser)\n",
    "# gl = generator(noise)\n",
    "# gl = convert_generator_output(gl)\n",
    "# yg = critic(gl)\n",
    "# generator.train_on_batch(noise, yg)\n",
    "\n",
    "# g_loss, yg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # size of the latent space\n",
    "# latent_dim = 50\n",
    "# # create the critic\n",
    "# critic = make_critic_model((num_features, 1))\n",
    "# # create the generator\n",
    "# generator = make_generator_model(latent_dim)\n",
    "# # create the gan\n",
    "# gan_model = make_gan(generator, critic, latent_dim)\n",
    "# # load image data\n",
    "\n",
    "# epochs = 10\n",
    "# # train model\n",
    "# critic_loss_real, critic_loss_fake, generator_loss = train(generator, critic, dataset, latent_dim, n_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(critic_loss_fake, label=\"crit\")\n",
    "# plt.plot(generator_loss, label=\"gen\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the generator and critic\n",
    "# def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=2, n_batch=64, n_critic=5):\n",
    "# \t# calculate the number of batches per training epoch\n",
    "# \tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "# \t# calculate the number of training iterations\n",
    "# \tn_steps = bat_per_epo * n_epochs\n",
    "# \t# calculate the size of half a batch of samples\n",
    "# \thalf_batch = int(n_batch / 2)\n",
    "# \t# lists for keeping track of loss\n",
    "# \tc1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "# \t# manually enumerate epochs\n",
    "# \tfor i in range(n_steps):\n",
    "# \t\t# update the critic more than the generator\n",
    "# \t\tc1_tmp, c2_tmp = list(), list()\n",
    "# \t\tfor _ in range(n_critic):\n",
    "# \t\t\t# get randomly selected 'real' samples\n",
    "# \t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "# \t\t\t# update critic model weights\n",
    "# \t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n",
    "# \t\t\tc1_tmp.append(c_loss1)\n",
    "# \t\t\t# generate 'fake' examples\n",
    "# \t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "# \t\t\t# update critic model weights\n",
    "# \t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
    "# \t\t\tc2_tmp.append(c_loss2)\n",
    "# \t\t# store critic loss\n",
    "# \t\tc1_hist.append(np.mean(c1_tmp))\n",
    "# \t\tc2_hist.append(np.mean(c2_tmp))\n",
    "# \t\t# prepare points in latent space as input for the generator\n",
    "# \t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
    "# \t\tprint(X_gan.shape)\n",
    "# \t\t# create inverted labels for the fake samples\n",
    "# \t\ty_gan = -np.ones((n_batch, 1))\n",
    "# \t\t# update the generator via the critic's error\n",
    "# \t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "# \t\tg_hist.append(g_loss)\n",
    "# \t\t# summarize loss on this batch\n",
    "# \t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n",
    "# \t\tplot_history(c1_hist, c2_hist, g_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ankh-morpork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
