{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 15:16:54.024939: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-16 15:16:55.060854: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-16 15:16:55.060874: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-16 15:16:55.066578: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-16 15:16:55.609456: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-16 15:16:55.610901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-16 15:16:58.138921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = pd.read_csv(\"/home/knel/virtual_envs/ankh-morpork/ICS_data_generation/data/swat_processed.csv\", sep=\",\", usecols=range(1,23), skiprows=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attacks = preprocessing.normalize(attacks.to_numpy(), norm=\"max\", axis=0)\n",
    "attacks = attacks.to_numpy()\n",
    "\n",
    "# swap sport to front\n",
    "attacks[:, [0, 1]] = attacks[:, [1, 0]]\n",
    "# swap dport to after sport\n",
    "attacks[:, [1, 2]] = attacks[:, [2, 1]]\n",
    "# swap protocols to after dport\n",
    "attacks[:, [2, 14]] = attacks[:, [14, 2]]\n",
    "\n",
    "# column order now -> sport, dport, protocols, continuous (discrete, discrete, discrete, continuous)\n",
    "\n",
    "\n",
    "\n",
    "# should probably add batch and shuffle\n",
    "\n",
    "train_dataset = attacks[:int(np.floor(attacks.shape[0]*3/4))]\n",
    "test_dataset = attacks[int(np.floor(attacks.shape[0]*3/4)):]\n",
    "\n",
    "num_features = attacks[:int(np.floor(attacks.shape[0]*3/4))].shape[1]\n",
    "seq_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1993625, 664542)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip model weights to a given hypercube\n",
    "class ClipConstraint(tf.keras.constraints.Constraint):\n",
    "\t# set clip value when initialized\n",
    "\tdef __init__(self, clip_value):\n",
    "\t\tself.clip_value = clip_value\n",
    "\n",
    "\t# clip model weights to hypercube\n",
    "\tdef __call__(self, weights):\n",
    "\t\treturn tf.keras.backend.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "\t# get the config\n",
    "\tdef get_config(self):\n",
    "\t\treturn {'clip_value': self.clip_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise will have same dimension as one flow\n",
    "\n",
    "def make_generator_model(input_dim):\n",
    "    \"\"\"\n",
    "    The generator takes noise as input and goes through a couple\n",
    "    of hidden layers. Then, it splits the output into 3 softmax layers\n",
    "    (one for each categorical variable with the same dimension as the \n",
    "    number of possible values that variable can take) and a dense\n",
    "    layer for all the rest of the continuous variables.\n",
    "\n",
    "    Bigger node size in the initial dense layer decreases critic loss substantially.\n",
    "\n",
    "    Adding sigmoid acativation to continuous output layer drops generator loss\n",
    "    (probably also really need this so it doesn't predict negative values).\n",
    "    \"\"\"\n",
    "    input = tf.keras.layers.Input(shape=input_dim, name=\"generator input\")\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(1000, name=\"generator_dense_1\")(input)\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(5000, name=\"generator_dense_2\")(hidden)\n",
    "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "    hidden = tf.keras.layers.LeakyReLU(0.2)(hidden)\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(10000, name=\"generator_input_3\")(hidden)\n",
    "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "    hidden = tf.keras.layers.LeakyReLU(0.2)(hidden)\n",
    "\n",
    "    sport_hidden = tf.keras.layers.Dense(23030, name=\"sport_hidden\")(hidden)\n",
    "    sport_hidden = tf.keras.layers.BatchNormalization()(sport_hidden)\n",
    "    sport_hidden = tf.keras.layers.LeakyReLU(0.2)(sport_hidden)\n",
    "\n",
    "    dport_hidden = tf.keras.layers.Dense(14372, name=\"dport_hidden\")(hidden)\n",
    "    dport_hidden = tf.keras.layers.BatchNormalization()(dport_hidden)\n",
    "    dport_hidden = tf.keras.layers.LeakyReLU(0.2)(dport_hidden)\n",
    "\n",
    "    proto_hidden = tf.keras.layers.Dense(7, name=\"proto_hidden\")(hidden)\n",
    "    proto_hidden = tf.keras.layers.BatchNormalization()(proto_hidden)\n",
    "    proto_hidden = tf.keras.layers.LeakyReLU(0.2)(proto_hidden)\n",
    "\n",
    "    # dense output for continuous, softmax for categorical\n",
    "    sport_output = tf.keras.layers.Softmax(1, name=\"sport_output\")(sport_hidden)\n",
    "    dport_output = tf.keras.layers.Softmax(1, name=\"dport_output\")(dport_hidden)\n",
    "    proto_output = tf.keras.layers.Softmax(1, name=\"proto_output\")(proto_hidden)\n",
    "    cont_output = tf.keras.layers.Dense(num_features-3, name=\"cont_output\", activation=\"tanh\")(hidden)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=[sport_output, dport_output, proto_output, cont_output], \n",
    "                           name=\"Generator\")\n",
    "    # opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "    # model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_generator_output(arr):\n",
    "    \"\"\"\n",
    "    For each categorical feature, takes the most likely probability given\n",
    "    by the Softmax output and concatenates it with the continuous features.\n",
    "    This is so the generated data has the same shape as the real data.\n",
    "    \"\"\"\n",
    "    sport_dist, dport_dist, proto_dist, cont = arr\n",
    "    num_samples = sport_dist.shape[0]\n",
    "    final = np.zeros(22)\n",
    "    for sample_index in range(num_samples):\n",
    "        sport = np.array([np.argmax(sport_dist[sample_index])+1])\n",
    "        dport = np.array([np.argmax(dport_dist[sample_index])+1])\n",
    "        proto = np.array([np.argmax(proto_dist[sample_index])+1])\n",
    "        data_point = np.concatenate((sport, dport, proto, cont[sample_index]))\n",
    "        final = np.vstack((final, data_point))\n",
    "    return tf.convert_to_tensor(final[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_generator_output_tensor(tens):\n",
    "    sport_dist, dport_dist, proto_dist, cont = tens\n",
    "    sport_dist = sport_dist.numpy()\n",
    "    dport_dist = dport_dist.numpy()\n",
    "    proto_dist = proto_dist.numpy()\n",
    "    cont = cont.numpy()\n",
    "    # num_samples = 5\n",
    "    # print(tf.shape(sport_dist))\n",
    "    # final = tf.zeros(num_features,)\n",
    "    # for sample_index in range(num_samples):\n",
    "    #     sport_val = tf.constant(tf.argmax(sport_dist[sample_index]).numpy(), dtype=tf.float32)\n",
    "    #     dport_val = tf.constant(tf.argmax(dport_dist[sample_index]).numpy(), dtype=tf.float32)\n",
    "    #     proto_val = tf.constant(tf.argmax(proto_dist[sample_index]).numpy(), dtype=tf.float32)\n",
    "    #     disc = tf.stack([sport_val, dport_val, proto_val], 0)\n",
    "    #     data_point = tf.concat([disc, cont[sample_index]], 0)\n",
    "    #     final = tf.stack([final, data_point], 1)\n",
    "    # print(final)\n",
    "    num_samples = sport_dist.shape[0]\n",
    "    final = np.zeros(22)\n",
    "    for sample_index in range(num_samples):\n",
    "        sport = np.array([np.argmax(sport_dist[sample_index])+1])\n",
    "        dport = np.array([np.argmax(dport_dist[sample_index])+1])\n",
    "        proto = np.array([np.argmax(proto_dist[sample_index])+1])\n",
    "        data_point = np.concatenate((sport, dport, proto, cont[sample_index]))\n",
    "        final = np.vstack((final, data_point))\n",
    "    return tf.convert_to_tensor(final[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = make_generator_model(50)\n",
    "# noise = generate_latent_points(50, 5)\n",
    "# X_fake = generator(noise, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(convert_generator_output(X_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_critic_model(input_shape):\n",
    "    \"\"\"\n",
    "    Critic (discriminator) more or less as described by the WGAN people.\n",
    "    The output layer uses linear activation since this is a critic and not \n",
    "    a standard discriminator.\n",
    "    \"\"\"\n",
    "    const = ClipConstraint(0.01)\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=input_shape, name=\"discriminator input\")\n",
    "    hidden = tf.keras.layers.LSTM(100, recurrent_dropout=0.4, return_sequences=True, kernel_constraint=const)(input)\n",
    "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "    hidden = tf.keras.layers.LSTM(100, recurrent_dropout=0.4, kernel_constraint=const)(hidden)\n",
    "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"linear\")(hidden)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output, name=\"Critic\")\n",
    "\n",
    "    # opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "    # model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gan(generator, critic, gen_input_dim):\n",
    "    \"\"\"\n",
    "    Would be a convenient way to train the GAN but this doesn't work\n",
    "    because the generator output has 4 tensors (one for each layer)\n",
    "    and the critic expects an input with the same dimension as the real data.\n",
    "\n",
    "    We can't just change the shape of the real data because I'd have to one-hot encode\n",
    "    the categorical variables - these datasets are huge and it would take FOREVER.\n",
    "    \"\"\"\n",
    "    for layer in critic.layers:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=gen_input_dim, name=\"combined input\")\n",
    "    x = generator(input)\n",
    "    x = critic(x)\n",
    "    model = tf.keras.Model(inputs=input, outputs=x)\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions that may or may not be useful\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t\"\"\"\n",
    "\tGenerate points in latent space as input for the generator.\n",
    "\t\"\"\"\n",
    "\t# generate points in the latent space\n",
    "\tx_input = np.random.randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t\"\"\"\n",
    "\tSelect real samples.\n",
    "\t\"\"\"\n",
    "\t# choose random instances\n",
    "\tix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "\t# select data\n",
    "\tX = dataset[ix]\n",
    "\t# refactor real data to have same shape as generated data (4 tensors)\n",
    "\t\n",
    "\t# generate class labels, -1 for 'real'\n",
    "\ty = -np.ones((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t\"\"\"\n",
    "\tUse the generator to generate n fake examples, with class labels\n",
    "\t\"\"\"\n",
    "\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\tX = convert_generator_output(X)\n",
    "\t# create class labels with 1.0 for 'fake'\n",
    "\ty = np.ones((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wasserstein loss for critic\n",
    "def critic_loss(pred_real, pred_fake):\n",
    "    # return tf.keras.backend.mean(pred_real * pred_fake)\n",
    "    real_loss = tf.reduce_mean(pred_real)\n",
    "    fake_loss = tf.reduce_mean(pred_fake)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "# Wasserstein loss for generator\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def generator_loss(pred_fake):\n",
    "    return -tf.keras.backend.mean(pred_fake)\n",
    "    # return cross_entropy(tf.ones_like(pred_fake), pred_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimiser = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "critic_optimiser = tf.keras.optimizers.RMSprop(learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(generator, critic, dataset, latent_dim, n_batch=64, n_critic=5):\n",
    "\n",
    "    for _ in range(n_critic):\n",
    "\n",
    "        \"\"\"\n",
    "        Train critic more often than the generator for WGAN\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            # Critic training works fine\n",
    "            X_real, y_real = generate_real_samples(dataset, n_batch)\n",
    "            noise = generate_latent_points(latent_dim, n_batch)\n",
    "            X_fake = generator(noise, training=True)\n",
    "            X_fake = convert_generator_output(X_fake)\n",
    "\n",
    "            pred_real = critic(X_real, training=True)\n",
    "            pred_fake = critic(X_fake, training=True)\n",
    "\n",
    "            c_loss = critic_loss(pred_real, pred_fake)\n",
    "        \n",
    "\n",
    "        critic_gradients = critic_tape.gradient(c_loss, critic.trainable_variables)\n",
    "        critic_optimiser.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "    print(f\"Critic loss: {c_loss}\")  \n",
    "    \"\"\" \n",
    "    Train generator\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        X_fake = generator(noise, training=True)\n",
    "        X_fake = convert_generator_output(X_fake)\n",
    "        pred_fake = critic(X_fake, training=False)\n",
    "        g_loss = generator_loss(pred_fake)\n",
    "        print(f\"Generator loss: {g_loss}\")\n",
    "    \n",
    "    # These gradients are all None and I can't figure out why\n",
    "    # Are the multiple output layers of the generator a problem here?\n",
    "    # Maybe the definition of the loss function?\n",
    "    generator_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    # generator_optimiser.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "\n",
    "\n",
    "    return c_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, n_epochs=2, latent_dim=50):\n",
    "\n",
    "    generator = make_generator_model(latent_dim)\n",
    "    critic = make_critic_model((num_features, 1))\n",
    "\n",
    "    c_losses, g_losses = [], []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        print(f\"Epoch: {i+1}\")\n",
    "        for batch in dataset:\n",
    "            c_loss, g_loss = train_step(generator, critic, batch, latent_dim)\n",
    "            c_losses.append(c_loss)\n",
    "            g_losses.append(g_loss)\n",
    "\n",
    "    return c_losses, g_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Critic loss: 5.960464477539063e-08\n",
      "Generator loss: 0.008305775001645088\n",
      "Critic loss: 2.682209014892578e-07\n",
      "Generator loss: 0.015814177691936493\n",
      "Critic loss: -1.1175870895385742e-07\n",
      "Generator loss: 0.021025950089097023\n",
      "Critic loss: 4.842877388000488e-08\n",
      "Generator loss: 0.024651803076267242\n",
      "Critic loss: 2.0116567611694336e-07\n",
      "Generator loss: 0.030883759260177612\n",
      "Critic loss: -2.2351741790771484e-08\n",
      "Generator loss: 0.03645201399922371\n",
      "Critic loss: -1.2665987014770508e-07\n",
      "Generator loss: 0.0397157296538353\n",
      "Critic loss: -2.2351741790771484e-08\n",
      "Generator loss: 0.04340490326285362\n",
      "Critic loss: -1.4901161193847656e-08\n",
      "Generator loss: 0.049316175282001495\n",
      "Critic loss: -1.4901161193847656e-08\n",
      "Generator loss: 0.05171509087085724\n",
      "Epoch: 2\n",
      "Critic loss: -5.960464477539063e-08\n",
      "Generator loss: 0.05397547036409378\n",
      "Critic loss: -4.470348358154297e-08\n",
      "Generator loss: 0.05959901586174965\n",
      "Critic loss: 5.21540641784668e-08\n",
      "Generator loss: 0.06110450625419617\n",
      "Critic loss: 1.7136335372924805e-07\n",
      "Generator loss: 0.060439012944698334\n",
      "Critic loss: 5.494803190231323e-08\n",
      "Generator loss: 0.06402260810136795\n",
      "Critic loss: -1.30385160446167e-08\n",
      "Generator loss: 0.06697080284357071\n",
      "Critic loss: -5.122274160385132e-08\n",
      "Generator loss: 0.06506535410881042\n",
      "Critic loss: 0.0\n",
      "Generator loss: 0.06527639925479889\n",
      "Critic loss: 1.564621925354004e-07\n",
      "Generator loss: 0.068697988986969\n",
      "Critic loss: -3.3527612686157227e-08\n",
      "Generator loss: 0.06521518528461456\n"
     ]
    }
   ],
   "source": [
    "# Test training on a small sample of the dataset\n",
    "dataset = np.array_split(attacks[:5000], 10)\n",
    "# dataset = (tf.data.Dataset.from_tensor_slices(attacks[:1000]).shuffle(10000).batch(64))\n",
    "c_losses, g_losses = train(dataset, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbc7c697dd0>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_losses)\n",
    "plt.plot(c_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "\n",
    "class WGAN(tf.keras.models.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            critic: tf.keras.models.Model,\n",
    "            generator: tf.keras.models.Model,\n",
    "            latent_dim: int,\n",
    "            critic_extra_steps: int=5,\n",
    "    ) -> None:\n",
    "        super(WGAN, self).__init__()\n",
    "        self.critic = critic\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.critic_extra_steps = critic_extra_steps\n",
    "\n",
    "    def compile(\n",
    "            self,\n",
    "            critic_optimiser: tf.keras.optimizers.Optimizer,\n",
    "            generator_optimiser: tf.keras.optimizers.Optimizer,\n",
    "            critic_loss: typing.Callable,\n",
    "            generator_loss: typing.Callable,\n",
    "            **kwargs\n",
    "    ) -> None:\n",
    "        super(WGAN, self).compile(**kwargs)\n",
    "        self.critic_optimiser = critic_optimiser\n",
    "        self.generator_optimiser = generator_optimiser\n",
    "        self.critic_loss = critic_loss\n",
    "        self.generator_loss = generator_loss\n",
    "\n",
    "    def train_step(self, databatch):\n",
    "        n_batch = 64\n",
    "        noise = generate_latent_points(50, n_batch)\n",
    "        \n",
    "        for _ in range(self.critic_extra_steps):\n",
    "\n",
    "            with tf.GradientTape() as critic_tape:\n",
    "                # Critic training works fine\n",
    "                # X_real, y_real = generate_real_samples(dataset, n_batch)\n",
    "                # X_fake = self.generator(noise, training=False)\n",
    "                X_fake = self.generator(noise, training=True)\n",
    "                X_fake = convert_generator_output_tensor(X_fake)\n",
    "\n",
    "                # pred_real = self.critic(X_real, training=True)\n",
    "                pred_real = self.critic(databatch, training=True)\n",
    "                pred_fake = self.critic(X_fake, training=True)\n",
    "\n",
    "                c_loss = self.critic_loss(pred_real, pred_fake)\n",
    "\n",
    "            critic_gradients = critic_tape.gradient(c_loss, self.critic.trainable_variables)\n",
    "            critic_optimiser.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        \"\"\" \n",
    "        Train generator\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            X_fake = self.generator(noise, training=True)\n",
    "            X_fake = convert_generator_output(X_fake)\n",
    "            pred_fake = self.critic(X_fake, training=True)\n",
    "            g_loss = generator_loss(pred_fake)\n",
    "\n",
    "        # These gradients are all None and I can't figure out why\n",
    "        # Are the multiple output layers of the generator a problem here?\n",
    "        # Maybe the definition of the loss function?\n",
    "        generator_gradients = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        generator_optimiser.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
    "\n",
    "        return c_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:23:04.195960: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 20000000000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "dataset = attacks[:1000]\n",
    "# dataset = (tf.data.Dataset.from_tensor_slices(attacks[:1000]).shuffle(10000).batch(64))\n",
    "\n",
    "generator = make_generator_model(100)\n",
    "critic = make_critic_model((num_features, 1))\n",
    "\n",
    "gan = WGAN(critic, generator, latent_dim=100, critic_extra_steps=5)\n",
    "gan.compile(critic_optimiser, generator_optimiser, critic_loss, generator_loss)\n",
    "gan.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = make_generator_model(50)\n",
    "# critic = make_critic_model((num_features, 1))\n",
    "# noise = generate_latent_points(50, 3)\n",
    "# X_fake = generator(noise, training=False)\n",
    "# X_fake = convert_generator_output(X_fake)\n",
    "# y_fake = critic(X_fake)\n",
    "# g_loss = generator_loss(y_fake)\n",
    "\n",
    "# generator.compile(loss=generator_loss, optimizer=generator_optimiser)\n",
    "# gl = generator(noise)\n",
    "# gl = convert_generator_output(gl)\n",
    "# yg = critic(gl)\n",
    "# generator.train_on_batch(noise, yg)\n",
    "\n",
    "# g_loss, yg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # size of the latent space\n",
    "# latent_dim = 50\n",
    "# # create the critic\n",
    "# critic = make_critic_model((num_features, 1))\n",
    "# # create the generator\n",
    "# generator = make_generator_model(latent_dim)\n",
    "# # create the gan\n",
    "# gan_model = make_gan(generator, critic, latent_dim)\n",
    "# # load image data\n",
    "\n",
    "# epochs = 10\n",
    "# # train model\n",
    "# critic_loss_real, critic_loss_fake, generator_loss = train(generator, critic, dataset, latent_dim, n_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(critic_loss_fake, label=\"crit\")\n",
    "# plt.plot(generator_loss, label=\"gen\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the generator and critic\n",
    "# def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=2, n_batch=64, n_critic=5):\n",
    "# \t# calculate the number of batches per training epoch\n",
    "# \tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "# \t# calculate the number of training iterations\n",
    "# \tn_steps = bat_per_epo * n_epochs\n",
    "# \t# calculate the size of half a batch of samples\n",
    "# \thalf_batch = int(n_batch / 2)\n",
    "# \t# lists for keeping track of loss\n",
    "# \tc1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "# \t# manually enumerate epochs\n",
    "# \tfor i in range(n_steps):\n",
    "# \t\t# update the critic more than the generator\n",
    "# \t\tc1_tmp, c2_tmp = list(), list()\n",
    "# \t\tfor _ in range(n_critic):\n",
    "# \t\t\t# get randomly selected 'real' samples\n",
    "# \t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "# \t\t\t# update critic model weights\n",
    "# \t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n",
    "# \t\t\tc1_tmp.append(c_loss1)\n",
    "# \t\t\t# generate 'fake' examples\n",
    "# \t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "# \t\t\t# update critic model weights\n",
    "# \t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
    "# \t\t\tc2_tmp.append(c_loss2)\n",
    "# \t\t# store critic loss\n",
    "# \t\tc1_hist.append(np.mean(c1_tmp))\n",
    "# \t\tc2_hist.append(np.mean(c2_tmp))\n",
    "# \t\t# prepare points in latent space as input for the generator\n",
    "# \t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
    "# \t\tprint(X_gan.shape)\n",
    "# \t\t# create inverted labels for the fake samples\n",
    "# \t\ty_gan = -np.ones((n_batch, 1))\n",
    "# \t\t# update the generator via the critic's error\n",
    "# \t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "# \t\tg_hist.append(g_loss)\n",
    "# \t\t# summarize loss on this batch\n",
    "# \t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n",
    "# \t\tplot_history(c1_hist, c2_hist, g_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ankh-morpork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
